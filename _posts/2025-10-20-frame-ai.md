---
layout: post
title: "Frame AI: Building an AI-Powered Photography Assistant"
date: 2025-10-20
description: "How I built an AI system that analyzes and enhances photos while teaching me."
author: "Bejay"
acknowledgment: "Built with curiosity, debugged with patience, polished with <span style='color: #3182ce; font-weight: 500;'>Claude</span>."
---

## Introduction: The Mobile Photography Itch

Every developer will tell you this: "I want to work on a side project to improve my portfolio." But almost all of them will also admit they never got around to building it. I wanted to break that loop, so I started hunting for ideas. I've always been fascinated by photography. Never professional-level good, but I can click half-decent pics on my iPhone. One of my friends introduced me to amateur photography principles — rule of thirds, leading lines, proper lighting. I try to keep those in mind while taking snaps. More often than not, I fail, lol. That's when the idea hit me: What if I could analyze images using vision LLMs (like gemini-2.5-flash) by prompting them correctly to check alignment with widely accepted photography rules? As I was iterating on the project, Google stealthily launched nano-banana — a revolution in closed-model image generation. I thought, why not edit the images based on the analysis? So yeah, in short: Frame AI analyzes images and critiques them, and you can enhance images using nano-banana. 

**Why build Frame AI as a side project:**
- Bridge the gap between taking photos and knowing how to improve them
- Explore the AI + photography intersection
- Build a real learning playground for system design

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/comparison.png" alt="Before and after photo comparison">
  <figcaption>A casual before/after comparison showing Frame AI's enhancement</figcaption>
</figure>

---

## What Frame AI Actually Does

**The core idea**: An AI assistant that understands photography principles.

Not just "make it prettier" - actual compositional feedback. There are fixed rules in photography: rule of thirds, leading lines, lighting, balance, and so on. Frame AI analyzes against these principles and suggests improvements.

**Two main features:**

1. **Analysis**: What's working, what's not, and why
2. **Enhancement**: AI-powered edits based on best practices

**The interesting twist**: Instructions generated separately -> So basically I make a separate call to LLM to generate 3 prompts from the analysis, each focusing on separate parts of the feedback. It also has access to the best practices of nano banana prompting techniques.

Earlier I was passing the analysis directly to the nano banana generation prompt but the output wasn't that good. My intuition was that nano banana is good at generating or editing images given it is given clear cut instructions. We shouldn't depend on it to reason and then generate. 

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/quick-analysis.png" alt="Sample analysis output">
  <figcaption>Digestible, bullet-pointed feedback that actually helps</figcaption>
</figure>

---

## System Design: How It All Fits Together

**High-level architecture:**

User uploads image → FastAPI backend → Image processing & caching layer → LLM analysis (with 3 separate generations) → Edit instruction generation → Response delivery

**Key components:**

- **Database**: Sqilite
- **LLM Integration**: Using Gemini Flash Models ("gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-2.5-flash-image")
- **Caching Strategy**: Why images being cached matters (performance + cost)
- **API Design**: RESTful endpoints for upload, analyze, edit

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/system-design.png" alt="System architecture diagram" data-lightbox="image">
  <figcaption>Clean architectural overview of Frame AI</figcaption>
</figure>

[Expand on each component and how they interact]

---

## Design Decisions & Lessons Learned

### Decision 1: Image Caching

Why it's critical: [Your reasoning about repeat analyses, cost optimization]

How I implemented it: [Your implementation details]

Performance gains observed: [Your metrics/observations]

### Decision 2: Three Separate Generations for Analysis

Why not just one? [Your reasoning]

Temperature, diversity, and picking the best output. [Your approach]

Trade-offs (cost vs. quality): [Your findings]

### Decision 3: Separate Call for Edit Instructions

Initial approach: Combined analysis + instructions

Why I split them: [Your reasoning]

Nano banana prompting technique: [Your explanation]

Results: More precise, actionable edits. [Your observations]

### Decision 4: What NOT to Do

- Don't add objects that weren't there (authenticity matters)
- Keep text digestible - no one reads walls of text
- Focus on enhancement, not transformation

### The Metric Saga

Added metrics tracking. Then removed it.

Why? [Your reasoning about feature bloat]

What I learned: [Your insights]

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/visual-diff.png" alt="Side-by-side showing what changed">
  <figcaption>Visual diff showing exactly what was edited</figcaption>
</figure>

---

## Technical Challenges & Solutions

**Challenge 1**: MIME type detection for various image formats
[Your solution]

**Challenge 2**: Handling large images efficiently
[Your solution]

**Challenge 3**: Balancing LLM call costs with quality
[Your solution]

**Challenge 4**: Making AI feedback actually useful (not generic)
[Your solution]

[Optional: Include code snippet or flowchart]

---

## What I Built vs. What I Learned

**The product**: A working AI photography assistant.

**The real wins:**
- Understanding LLM prompting nuances
- System design for AI-powered apps
- Caching strategies that actually matter
- Iteration over perfection
- Knowing when to remove features

[Your personal reflections]

---

## What's Next

Potential improvements:
- Batch processing
- Style preferences/learning
- Mobile app integration
- Community sharing features

Open questions:
- How to balance automation with creative control?
- What makes AI feedback feel "authentic" vs. generic?

---

## Conclusion: The Side Project Effect

Started wanting to improve my photos. Ended up learning system design, LLM engineering, and product iteration.

Frame AI isn't just a tool - it's a learning artifact.

[Your closing thoughts]

**Try it, break it, let me know what you think**: [Your contact/link]

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/hero-app.png" alt="Frame AI in action">
  <figcaption>Frame AI in action</figcaption>
</figure>
