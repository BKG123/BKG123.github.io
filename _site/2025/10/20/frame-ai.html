<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Frame AI: Building an AI-Powered Photography Assistant | Bejay’s Website</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Frame AI: Building an AI-Powered Photography Assistant" />
<meta name="author" content="Bejay" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I built an AI system that analyzes and enhances photos while teaching me." />
<meta property="og:description" content="How I built an AI system that analyzes and enhances photos while teaching me." />
<link rel="canonical" href="http://localhost:4000/2025/10/20/frame-ai.html" />
<meta property="og:url" content="http://localhost:4000/2025/10/20/frame-ai.html" />
<meta property="og:site_name" content="Bejay’s Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-20T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Frame AI: Building an AI-Powered Photography Assistant" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Bejay"},"dateModified":"2025-10-20T00:00:00+05:30","datePublished":"2025-10-20T00:00:00+05:30","description":"How I built an AI system that analyzes and enhances photos while teaching me.","headline":"Frame AI: Building an AI-Powered Photography Assistant","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/10/20/frame-ai.html"},"url":"http://localhost:4000/2025/10/20/frame-ai.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="icon" type="image/x-icon" href="/assets/images/favicons/favicon.ico">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicons/favicon-16x16.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicons/apple-touch-icon.png">
    <link rel="manifest" href="/assets/images/favicons/site.webmanifest">
    <link rel="stylesheet" href="/assets/css/style.css?v=d9406b1c031f3ecafce21178c9c19de31b2c51d4">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  </head>
  <body>
    <div class="scroll-progress"></div>
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <div class="container">
      <header class="site-header" role="banner">
  <nav class="site-nav" role="navigation" aria-label="Main navigation">
    <div class="nav-brand">
      <a href="/" class="brand-link">BKG</a>
    </div>
    <div class="nav-menu">
      <a href="/" class="nav-link">About</a>
      <a href="/blog/" class="nav-link">Blog</a>
      <a href="/projects/" class="nav-link">Projects</a>
      <button class="theme-toggle" aria-label="Toggle theme">
        <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <circle cx="12" cy="12" r="5"></circle>
          <line x1="12" y1="1" x2="12" y2="3"></line>
          <line x1="12" y1="21" x2="12" y2="23"></line>
          <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
          <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
          <line x1="1" y1="12" x2="3" y2="12"></line>
          <line x1="21" y1="12" x2="23" y2="12"></line>
          <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
          <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
        </svg>
        <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
        </svg>
      </button>
    </div>
  </nav>
</header>

      <main id="main-content" class="main-content" role="main">
        <div style="text-align: center; margin-bottom: 2rem;">
    <a href="/blog/" style="color: #3182ce; text-decoration: none; font-weight: 500;">← Back to Blog</a>
</div>

<article class="content-section">
    <header class="post-header">
        <h1 class="post-title">Frame AI: Building an AI-Powered Photography Assistant</h1>
        <p class="post-meta">
            <time datetime="2025-10-20T00:00:00+05:30">October 20, 2025</time>
             • by Bejay
        </p>
    </header>

    <div class="post-content">
        <h2 id="introduction-the-mobile-photography-itch">Introduction: The Mobile Photography Itch</h2>

<p>Every developer will tell you this: “I want to work on a side project to improve my portfolio.” But almost all of them will also admit they never got around to building it. I wanted to break that loop, so I started hunting for ideas. I’ve always been fascinated by photography. Never professional-level good, but I can click half-decent pics on my iPhone. One of my friends introduced me to amateur photography principles — rule of thirds, leading lines, proper lighting. I try to keep those in mind while taking snaps. More often than not, I fail, lol. That’s when the idea hit me: What if I could analyze images using vision LLMs (like Gemini 2.5 Flash) by prompting them correctly to check alignment with widely accepted photography rules? As I was iterating on the project, Google launched Gemini 2.5 Flash Image (nicknamed “nano-banana” by the developer community) in August 2025 — a breakthrough in image generation and editing that hit #1 on LMArena’s leaderboards. I thought, why not edit the images based on the analysis? So yeah, in short: Frame AI analyzes images and critiques them, and you can enhance images using Gemini 2.5 Flash Image.</p>

<p><strong>Try it here</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a><br />
<strong>GitHub</strong>: <a href="https://github.com/BKG123/frame-ai" target="_blank" rel="noopener noreferrer">https://github.com/BKG123/frame-ai</a><br />
<strong>Video Demo</strong>: <a href="/projects/frame-ai/">Watch it in action →</a></p>

<p><strong>Why build Frame AI as a side project:</strong></p>
<ul>
  <li>Bridge the gap between taking photos and knowing how to improve them</li>
  <li>Explore the AI + photography intersection</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/app-1.webp" alt="Sample app image" />
  <figcaption>Frame AI Home</figcaption>
</figure>

<hr />

<h2 id="what-frame-ai-actually-does">What Frame AI Actually Does</h2>

<p>So the core idea is simple: an AI assistant that actually understands photography principles.</p>

<p>Not just “make it prettier” - I wanted actual compositional feedback. There are fixed rules in photography: rule of thirds, leading lines, lighting, balance, and so on. Frame AI analyzes your photo against these principles and tells you what could be better.</p>

<p><strong>Two main things it does:</strong></p>

<ol>
  <li><strong>Analysis</strong>: What’s working, what’s not, and why</li>
  <li><strong>Enhancement</strong>: AI-powered edits based on the analysis</li>
</ol>

<p><strong>The interesting twist</strong>: Instructions generated separately -&gt; So basically I make a separate call to the LLM to generate 3 distinct editing prompts from the analysis, each focusing on different aspects: technical perfection, atmospheric reinterpretation, and conceptual narrative. It also has access to the best practices of Gemini 2.5 Flash Image prompting techniques.</p>

<p>Earlier I was passing the analysis directly to the image generation prompt but the output wasn’t that good. My intuition was that Gemini 2.5 Flash Image excels at generating or editing images when given clear, specific instructions. I shouldn’t depend on it to reason and then generate — separation of concerns works better.</p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/quick-analysis.webp" alt="Sample analysis output" />
  <figcaption>Digestible, bullet-pointed feedback that actually helps</figcaption>
</figure>

<hr />

<h2 id="system-design-how-it-all-fits-together">System Design: How It All Fits Together</h2>

<p><strong>High-level architecture:</strong></p>

<p>User uploads image → FastAPI backend → Image processing &amp; caching layer → LLM analysis → Stored in DB → Enhance Image Trigger → 3 editing prompts generated from analysis → 3 images generated in parallel using Gemini 2.5 Flash Image</p>

<p><strong>Key components:</strong></p>

<ul>
  <li><strong>Database</strong>: SQLite</li>
  <li><strong>LLM Integration</strong>: Gemini 2.5 Flash for analysis, Gemini 2.5 Flash Lite for JSON structuring, Gemini 2.5 Flash Image for enhancement</li>
  <li><strong>Caching Strategy</strong>: Content-based hashing prevents duplicate processing (performance + cost optimization)</li>
  <li><strong>API Design</strong>: RESTful endpoints with Server-Sent Events for streaming analysis</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/system-design.webp" alt="System architecture diagram" data-lightbox="image" />
  <figcaption>Clean architectural overview of Frame AI</figcaption>
</figure>

<p>As mentioned earlier, the product has two main parts: image analysis and image enhancement.</p>

<p><strong>Image Analysis Flow:</strong></p>
<ol>
  <li>Frontend calls <code class="language-plaintext highlighter-rouge">/upload</code> API with image file</li>
  <li>Backend workflow:
    <ul>
      <li>Content-based hash is generated from file bytes (fixes duplicate detection edge cases)</li>
      <li>Image stored permanently in <code class="language-plaintext highlighter-rouge">static/uploaded_images/{hash}.{ext}</code> (deduplication happens here)</li>
      <li>Check SQLite cache — if hash exists, stream cached analysis immediately</li>
      <li>For new images:
        <ul>
          <li>Temporary file created for processing</li>
          <li>Gemini 2.5 Flash call with image + photography best practices prompt</li>
          <li>LLM streams detailed analysis with scores (exposure, composition, lighting, overall)</li>
          <li>Analysis stored in SQLite against file hash</li>
          <li>Frontend receives analysis via Server-Sent Events (real-time streaming)</li>
        </ul>
      </li>
      <li>EXIF data extracted and stored for context</li>
    </ul>
  </li>
</ol>

<p><strong>Image Enhancement Flow:</strong></p>
<ol>
  <li>Frontend calls <code class="language-plaintext highlighter-rouge">/image/edit</code> API with file hash</li>
  <li>Backend workflow:
    <ul>
      <li>Fetch cached analysis from SQLite using hash</li>
      <li>Generate 3 distinct editing prompts via Gemini 2.5 Flash (with Gemini 2.5 Flash Image best practices as context)
        <ul>
          <li>Prompt 1: Technical perfection &amp; enhancement</li>
          <li>Prompt 2: Atmospheric &amp; mood reinterpretation</li>
          <li>Prompt 3: Conceptual &amp; narrative composite</li>
        </ul>
      </li>
      <li>Launch 3 parallel Gemini 2.5 Flash Image API calls (<code class="language-plaintext highlighter-rouge">asyncio.gather</code>)</li>
      <li>Each call returns: enhanced image + text description of changes</li>
      <li>Text descriptions converted to structured JSON via Gemini 2.5 Flash Lite (temperature = 0 for consistency)</li>
      <li>Return 3 enhanced images with metadata to frontend</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="design-decisions--lessons-learned">Design Decisions &amp; Lessons Learned</h2>

<h3 id="decision-1-content-based-hashing-for-caching">Decision 1: Content-Based Hashing for Caching</h3>

<p><strong>Why I needed this:</strong></p>
<ul>
  <li>The analysis needs to be reused for enhancement (same image = same analysis)</li>
  <li>Duplicate uploads waste API calls and cost me money</li>
  <li>Better UX: instant results if someone already uploaded that image</li>
</ul>

<p><strong>How I figured this out:</strong></p>
<ul>
  <li><strong>First attempt</strong>: Filename + IP-based caching
    <ul>
      <li>Problem: Same filename, different image = cache collision (oops)</li>
      <li>Problem: Storing IP addresses = PII concerns (not great)</li>
    </ul>
  </li>
  <li><strong>Final solution</strong>: Content-based hashing (SHA-256 of file bytes)
    <ul>
      <li>Same image content → same hash → reliable cache hit</li>
      <li>Different images → different hashes → no false positives</li>
      <li>Privacy-friendly: no PII stored</li>
    </ul>
  </li>
</ul>

<h3 id="decision-2-three-parallel-image-variations">Decision 2: Three Parallel Image Variations</h3>

<p><strong>Why not just one enhanced image?</strong></p>
<ul>
  <li>Photography enhancement is super subjective</li>
  <li>The analysis covers multiple things (technical stuff, artistic vibes, mood)</li>
  <li>People want choice — what looks “better” varies by taste and what you’re using it for</li>
  <li>Three variations let you see different creative directions</li>
</ul>

<p><strong>The three approaches I settled on:</strong></p>
<ol>
  <li><strong>Technical perfection</strong>: Fix exposure, sharpen details, recover dynamic range</li>
  <li><strong>Atmospheric reinterpretation</strong>: Transform mood through color grading and lighting</li>
  <li><strong>Conceptual narrative</strong>: Reimagine the story (subtle compositing, creative edits)</li>
</ol>

<p><strong>Hyperparameter tuning:</strong></p>
<ul>
  <li><strong>Analysis LLM</strong> (Gemini 2.5 Flash): temperature = 0.3-0.5 (balance creativity with structure)</li>
  <li><strong>JSON structuring</strong> (Gemini 2.5 Flash Lite): temperature = 0 (deterministic output)</li>
  <li><strong>Prompt generation</strong> (Gemini 2.5 Flash): temperature = 0.5 (creative but focused)</li>
</ul>

<h3 id="decision-3-separate-llm-call-for-prompt-generation">Decision 3: Separate LLM Call for Prompt Generation</h3>

<p><strong>What I tried first</strong>: Passing the analysis directly to Gemini 2.5 Flash Image</p>
<ul>
  <li>Problem: Output quality was all over the place</li>
  <li>The image model struggled to pull out actionable edits from my verbose analysis</li>
</ul>

<p><strong>What actually worked</strong>: Dedicated prompt generation step</p>
<ul>
  <li>Generate 3 specific editing prompts via Gemini 2.5 Flash</li>
  <li>Include <a href="https://ai.google.dev/gemini-api/docs/image-generation#best-practices" target="_blank" rel="noopener noreferrer">Gemini 2.5 Flash Image best practices</a> as context</li>
  <li>Pass clean, focused instructions to the image model</li>
</ul>

<p><strong>Why this works better</strong>: Gemini 2.5 Flash Image is great at following clear, step-by-step instructions — but I shouldn’t ask it to reason about photography theory <em>and</em> generate images at the same time. Separation of concerns wins again.</p>

<p><strong>Results</strong>: Way more precise, actionable edits. The enhancements became subtle but actually comprehensible instead of over-processed.</p>

<h3 id="decision-4-design-constraints--negative-prompting">Decision 4: Design Constraints &amp; Negative Prompting</h3>

<p><strong>What I explicitly told the models NOT to do:</strong></p>
<ul>
  <li>Don’t add objects/people that weren’t in the original (authenticity matters for photography)</li>
  <li>Don’t rotate or change orientation (preserve the photographer’s intent)</li>
  <li>Don’t over-process to the point of looking fake</li>
</ul>

<p><strong>Some UX stuff I care about:</strong></p>
<ul>
  <li>Keep feedback digestible — no one reads walls of text</li>
  <li>Focus on enhancement, not transformation</li>
  <li>Make AI feedback feel specific, not generic</li>
</ul>

<h3 id="trade-offs-cost-vs-quality">Trade-offs: Cost vs. Quality</h3>

<p><strong>My current approach</strong>: Optimize for quality first</p>
<ul>
  <li>I’m making multiple LLM calls per request (analysis + prompt generation + JSON structuring)</li>
  <li>Not worrying about cost during initial development</li>
  <li>Philosophy: Get the best results first, then optimize later</li>
</ul>

<p><strong>Future optimizations I’m thinking about:</strong></p>
<ul>
  <li>Replace JSON-generating LLM calls with markdown parsers</li>
  <li>Use Python imaging libraries (Pillow) for simple adjustments instead of hitting the image generation API</li>
  <li>Maybe fine-tune an open-source model for analysis</li>
  <li>Batch processing for multiple images</li>
</ul>

<h3 id="the-metric-saga">The Metric Saga</h3>

<p><strong>What happened</strong>: I added metrics tracking. Then I removed it.</p>

<p><strong>Why?</strong></p>
<ul>
  <li>I tried to give some quantitative insights using LLMs to calculate metrics like sharpness, color composition</li>
  <li>Problem: The generated image is completely new, not a modified version of the original</li>
  <li>So metrics might show “improvement” but they’re comparing apples to oranges - not super useful</li>
</ul>

<div class="image-grid">
  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/og_image.webp" alt="Original photo" />
    <figcaption>Original</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var1.webp" alt="Enhanced variation 1" />
    <figcaption>Variation 1</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var2.webp" alt="Enhanced variation 2" />
    <figcaption>Variation 2</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var3.webp" alt="Enhanced variation 3" />
    <figcaption>Variation 3</figcaption>
  </figure>
</div>

<hr />

<h2 id="some-challenges--solutions">Some Challenges &amp; Solutions</h2>

<h3 id="front-end-development">Front end development</h3>

<p><strong>Problem</strong>: I know very little FE dev</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>In this day and age you can’t really bracket yourself into FE and BE. And also I couldn’t really show my product via APIs!</li>
  <li>Claude Code came to the rescue. It really spun up the FE based on my instructions.
There were multiple iterations in a loop of claude doing something in FE -&gt; Me not liking it -&gt; putting ss in the input and asking it to rebuild. Also used Cursor’s in built browser tool to let it gather info of current FE design and make some tweaks.</li>
  <li>Attached the index.html file to Gemini and chatgpt and asked them to make it better but it was more or less the same thing.</li>
  <li>I felt that the design was not that great so used lovable to make some changes. Turns out, you can’t load an existing repo there. So created a dummy repo and pushed my code base there, connected to lovable. It couldn’t preview, but it added some animations to the buttons.</li>
  <li>The current state of the FE is not great but it gets the job done</li>
</ul>

<h3 id="orientation">Orientation</h3>

<p><strong>Problem</strong>:</p>
<ul>
  <li>The edited image came out in incorrect orientation because probably, iPhone photos (with which I was testing) had incorrect EXIF rotation metadata</li>
</ul>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Used Pillow’s <code class="language-plaintext highlighter-rouge">ImageOps.exif_transpose()</code> to auto-correct orientation before processing</li>
  <li>Explicitly added in the prompt to maintain orientation</li>
  <li>Both of these changes were suggested by Claude code and applied them at the same time -&gt; seemed to fix the problem</li>
</ul>

<h3 id="making-ai-feedback-actually-useful-not-generic">Making AI Feedback Actually Useful (Not Generic)</h3>

<p><strong>Problem</strong>: Early versions gave generic feedback like “nice composition” or “good lighting” that didn’t help users improve.</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Crafted a detailed system prompt with specific photography principles (rule of thirds, leading lines, etc.)</li>
  <li>Included EXIF data in the prompt so the LLM could explain technical settings</li>
  <li>Asked for structured feedback: strengths, improvements, professional tips</li>
  <li>Added numerical scores (1-10) for exposure, composition, lighting, overall</li>
  <li>I arrived at this after multiple iterations.</li>
</ul>

<p><strong>Example of specific vs. generic feedback</strong>:</p>
<ul>
  <li>❌ Generic: “The lighting could be better”</li>
  <li>✅ Specific: “The main light source creates harsh shadows on the subject’s face. Try shooting during the ‘golden hour’ or using a diffuser to soften the light.”</li>
</ul>

<h3 id="evals-planned">Evals (planned)</h3>
<ul>
  <li>This actually is an extension of the previous point.</li>
  <li>Till now I haven’t written ‘LLM Evals’ as such. Have some test cases which checks whether the json structure expected and some heuristics on length.</li>
  <li>Have plans to write proper evals for analysis - using LLM as a Judge method (have used this at work)</li>
  <li>Have some ideas about image evals from <a href="https://github.com/Storia-AI/image-eval" target="_blank" rel="noopener noreferrer">Storia-AI</a></li>
  <li>Will update this blog, after implementing evals</li>
</ul>

<h2 id="whats-next">What’s Next</h2>

<p><strong>Some improvements I want to make:</strong></p>
<ul>
  <li><strong>Evals</strong>: Any LLM-based app should have proper evals - can’t just depend on eyeballing things</li>
  <li><strong>Cost optimization</strong>: Consolidate LLM calls, maybe use open-source models for some tasks</li>
  <li><strong>Batch processing</strong>: Upload multiple photos, get bulk analysis</li>
  <li><strong>Feedback over time</strong>: Track a user’s uploads and help them get better at photography over time</li>
</ul>

<p><strong>Open questions I’m still thinking about:</strong></p>
<ul>
  <li>How do I balance automation with creative control? (Should I let users tweak prompts? Add sliders for “enhancement intensity”?)</li>
  <li>What makes AI feedback feel “authentic” vs. generic? (Still experimenting with prompt engineering)</li>
  <li>Should I add a “learn from feedback” loop where users mark helpful vs. unhelpful critiques?</li>
</ul>

<hr />

<h2 id="conclusion-the-side-project-effect">Conclusion: The Side Project Effect</h2>

<p>Now that I’ve shipped my first “Side Project”, I hope to be consistent and keep building new things at regular intervals. And also keep improving this one, obviously.</p>

<p><strong>Try it, break it, let me know what you think</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a></p>

<hr />

<h2 id="more-from-my-blog">More from my blog</h2>

<ul>
  <li><strong><a href="/2025/10/30/reel-craft.html">ReelCraft: AI-Powered Article to Video Pipeline</a></strong> - How I built a system that transforms articles into engaging short-form videos</li>
  <li><strong><a href="/2025/10/16/life-is-linear-regression.html">Everything in Life is Linear Regression</a></strong> - Why life’s complexities are best understood as weighted combinations of multiple factors</li>
  <li><strong><a href="/2025/09/21/coding-in-the-era-of-llms.html">Coding in the era of LLMs</a></strong> - My thoughts on AI-assisted coding and the importance of learning fundamentals</li>
  <li><strong><a href="/2025/04/15/your-feelings-lie-to-you-sometimes.html">Your Feelings Lie to You (Sometimes)</a></strong> - My exploration of how emotions and logic shape our decision-making process</li>
</ul>

    </div>

    
    <div style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e2e8f0; font-size: 0.9rem; color: #666; text-align: center; font-style: italic;">
        Built with curiosity, debugged with patience, polished with <span style='color: #3182ce; font-weight: 500;'>Claude</span>.
    </div>
    
</article>

<div style="text-align: center; margin-top: 2rem;">
    <p style="color: #666;">Thanks for reading! ✨</p>
</div>
      </main>

      <div class="feedback-link-container">
  <a href="https://docs.google.com/forms/d/e/1FAIpQLSfbU1WdeuTf4fFwC_rk4_iMbHpJmB3dFhBUAt4_R9k0ecTy0g/viewform" target="_blank" rel="noopener noreferrer" class="feedback-link">Leave Feedback</a>
</div>


<footer class="site-footer" role="contentinfo">
  <div class="footer-content">
    <div class="footer-main">
      <div class="footer-brand">
        <h3>BKG</h3>
        <p>Building intelligent systems and scalable solutions with a passion for Gen AI.</p>
      </div>
      <div class="footer-links">
        <div class="footer-section">
          <h4>Navigation</h4>
          <ul>
            <li><a href="/">About</a></li>
            <li><a href="/blog/">Blog</a></li>
            <li><a href="/projects/">Projects</a></li>
          </ul>
        </div>
        <div class="footer-section">
          <h4>Connect</h4>
          <ul>
            <li><a href="mailto:bejay.ketan1@gmail.com">Email</a></li>
            <li><a href="https://github.com/BKG123" target="_blank" rel="noopener noreferrer">GitHub</a></li>
            <li><a href="https://www.linkedin.com/in/bejay-ketan-guin-67970018a" target="_blank" rel="noopener noreferrer">LinkedIn</a></li>
            <li><a href="https://x.com/bkguin" target="_blank" rel="noopener noreferrer">Twitter</a></li>
          </ul>
        </div>
      </div>
    </div>
    <div class="footer-divider"></div>
    <div class="footer-bottom">
      <p class="footer-copyright">© 2025 Bejay Ketan Guin. All rights reserved.</p>
      <div class="footer-social">
        <a href="https://github.com/BKG123" target="_blank" class="footer-social-icon" aria-label="GitHub">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
            <path d="M12 0C5.374 0 0 5.373 0 12 0 17.302 3.438 21.8 8.207 23.387c.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/>
          </svg>
        </a>
        <a href="https://www.linkedin.com/in/bejay-ketan-guin-67970018a" target="_blank" class="footer-social-icon" aria-label="LinkedIn">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
            <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
          </svg>
        </a>
        <a href="https://x.com/bkguin" target="_blank" class="footer-social-icon" aria-label="Twitter">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
            <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
          </svg>
        </a>
        <a href="mailto:bejay.ketan1@gmail.com" class="footer-social-icon" aria-label="Email">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
            <polyline points="22,6 12,13 2,6"></polyline>
          </svg>
        </a>
      </div>
    </div>
  </div>
</footer>
    </div>

    <script src="/assets/js/theme.js"></script>
  </body>
</html>