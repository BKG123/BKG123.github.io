<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Frame AI: Building an AI-Powered Photography Assistant | Bejay‚Äôs Website</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Frame AI: Building an AI-Powered Photography Assistant" />
<meta name="author" content="Bejay" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I built an AI system that analyzes and enhances photos while teaching me." />
<meta property="og:description" content="How I built an AI system that analyzes and enhances photos while teaching me." />
<link rel="canonical" href="http://localhost:4000/2025/10/20/frame-ai.html" />
<meta property="og:url" content="http://localhost:4000/2025/10/20/frame-ai.html" />
<meta property="og:site_name" content="Bejay‚Äôs Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-20T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Frame AI: Building an AI-Powered Photography Assistant" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Bejay"},"dateModified":"2025-10-20T00:00:00+05:30","datePublished":"2025-10-20T00:00:00+05:30","description":"How I built an AI system that analyzes and enhances photos while teaching me.","headline":"Frame AI: Building an AI-Powered Photography Assistant","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/10/20/frame-ai.html"},"url":"http://localhost:4000/2025/10/20/frame-ai.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=77207e2b263414f1dbccaf39ff94a6ff1df72855">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  </head>
  <body>
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <div class="container">
      <header class="site-header" role="banner">
  <nav class="top-nav" role="navigation" aria-label="Main navigation">
    <a href="/" class="nav-link">about</a>
    <a href="/blog/" class="nav-link">blog</a>
    <a href="/projects/" class="nav-link">projects</a>
    <div class="nav-icons">
      <span class="theme-icon">üåô</span>
    </div>
  </nav>
</header>

      <main id="main-content" class="main-content" role="main">
        <div style="text-align: center; margin-bottom: 2rem;">
    <a href="/blog/" style="color: #3182ce; text-decoration: none; font-weight: 500;">‚Üê Back to Blog</a>
</div>

<article class="content-section">
    <header class="post-header">
        <h1 class="post-title">Frame AI: Building an AI-Powered Photography Assistant</h1>
        <p class="post-meta">
            <time datetime="2025-10-20T00:00:00+05:30">October 20, 2025</time>
             ‚Ä¢ by Bejay
        </p>
    </header>

    <div class="post-content">
        <h2 id="introduction-the-mobile-photography-itch">Introduction: The Mobile Photography Itch</h2>

<p>Every developer will tell you this: ‚ÄúI want to work on a side project to improve my portfolio.‚Äù But almost all of them will also admit they never got around to building it. I wanted to break that loop, so I started hunting for ideas. I‚Äôve always been fascinated by photography. Never professional-level good, but I can click half-decent pics on my iPhone. One of my friends introduced me to amateur photography principles ‚Äî rule of thirds, leading lines, proper lighting. I try to keep those in mind while taking snaps. More often than not, I fail, lol. That‚Äôs when the idea hit me: What if I could analyze images using vision LLMs (like gemini-2.5-flash) by prompting them correctly to check alignment with widely accepted photography rules? As I was iterating on the project, Google stealthily launched nano-banana ‚Äî a revolution in closed-model image generation. I thought, why not edit the images based on the analysis? So yeah, in short: Frame AI analyzes images and critiques them, and you can enhance images using nano-banana.</p>

<p><strong>Why build Frame AI as a side project:</strong></p>
<ul>
  <li>Bridge the gap between taking photos and knowing how to improve them</li>
  <li>Explore the AI + photography intersection</li>
  <li>Build a real learning playground for system design</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/app-1.png" alt="Sample app image" />
  <figcaption>Frame AI Home</figcaption>
</figure>

<hr />

<h2 id="what-frame-ai-actually-does">What Frame AI Actually Does</h2>

<p><strong>The core idea</strong>: An AI assistant that understands photography principles.</p>

<p>Not just ‚Äúmake it prettier‚Äù - actual compositional feedback. There are fixed rules in photography: rule of thirds, leading lines, lighting, balance, and so on. Frame AI analyzes against these principles and suggests improvements.</p>

<p><strong>Two main features:</strong></p>

<ol>
  <li><strong>Analysis</strong>: What‚Äôs working, what‚Äôs not, and why</li>
  <li><strong>Enhancement</strong>: AI-powered edits based on best practices</li>
</ol>

<p><strong>The interesting twist</strong>: Instructions generated separately -&gt; So basically I make a separate call to LLM to generate 3 prompts from the analysis, each focusing on separate parts of the feedback. It also has access to the best practices of nano banana prompting techniques.</p>

<p>Earlier I was passing the analysis directly to the nano banana generation prompt but the output wasn‚Äôt that good. My intuition was that nano banana is good at generating or editing images given it is given clear cut instructions. We shouldn‚Äôt depend on it to reason and then generate.</p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/quick-analysis.png" alt="Sample analysis output" />
  <figcaption>Digestible, bullet-pointed feedback that actually helps</figcaption>
</figure>

<hr />

<h2 id="system-design-how-it-all-fits-together">System Design: How It All Fits Together</h2>

<p><strong>High-level architecture:</strong></p>

<p>User uploads image ‚Üí FastAPI backend ‚Üí Image processing &amp; caching layer ‚Üí LLM analysis ‚Üí Stored in DB -&gt; Enhance Image Trigger ‚Üí 3 prompts generated for analysis ‚Üí 3 Images generated using nano-banana</p>

<p><strong>Key components:</strong></p>

<ul>
  <li><strong>Database</strong>: Sqlite</li>
  <li><strong>LLM Integration</strong>: Using Gemini Flash Models (‚Äúgemini-2.5-flash‚Äù, ‚Äúgemini-2.5-flash-lite‚Äù, ‚Äúgemini-2.5-flash-image‚Äù)</li>
  <li><strong>Caching Strategy</strong>: Why images being cached matters (performance + cost)</li>
  <li><strong>API Design</strong>: RESTful endpoints for upload, analyze, edit</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/system-design.png" alt="System architecture diagram" data-lightbox="image" />
  <figcaption>Clean architectural overview of Frame AI</figcaption>
</figure>

<p>As mentioned earlier the product has two main parts - image analysis and image enhancements.</p>

<p>Image Analysis:</p>
<ul>
  <li>
    <p>FE calls the upload api with image file</p>
  </li>
  <li>
    <p>In BE</p>
    <ul>
      <li>A hash is created using the contents of the file.</li>
      <li>The image file is stored in memory with the hash as the filename.</li>
      <li>A temporary file is created for analysis.</li>
      <li>A LLM call to gemini is made (‚Äúgemini-2.5-flash‚Äù) with the image as input and best practices of photography in the prompt</li>
      <li>The LLM outputs a detailed analysis along with scores.</li>
      <li>This ‚ÄúDetailed Analysis‚Äù is parsed and shown in the frontend.</li>
      <li>Another LLM call is made to output a ‚ÄúQuick Analysis‚Äù which is shown in a diff tab</li>
      <li>Both the analyses are stored in SQLite db against the file hash and the value is returned to FE, along with the file hash</li>
    </ul>
  </li>
</ul>

<p>Image Enhancement</p>
<ul>
  <li>
    <p>FE calls the image enhancement API with file hash</p>
  </li>
  <li>
    <p>In BE</p>
    <ul>
      <li>The analysis is fetched using the file hash.</li>
      <li>From the analysis, 3 prompts are generated using LLM call (nano banana best practices are passed as context)</li>
      <li>Using the 3 prompts, 3 parallel calls are made to nano banana api to generate 3 images and corresponding text outputs saying what changes were made</li>
      <li>These text outputs are passed through 3 llm calls to structure the output into json</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="design-decisions--lessons-learned">Design Decisions &amp; Lessons Learned</h2>

<h3 id="decision-1-image-caching">Decision 1: Image Caching</h3>

<p>Why it‚Äôs critical:</p>
<ul>
  <li>Caching of analysis is mostly necessary as I wanted to reuse it for enhancement.</li>
</ul>

<p>How I implemented it:</p>
<ul>
  <li>Frist tried with only filename + ip based caching but it was not that robust for same filename different image cases. Also didn‚Äôt want to store PII</li>
</ul>

<h3 id="decision-2-three-separate-generations-for-analysis">Decision 2: Three Separate Generations for Analysis</h3>

<p>Why not just one?</p>
<ul>
  <li>Enhancement of image is highly subjective, so it‚Äôs better to have 3 different outputs instead of 1. Also the analysis focuses on multiple params and its better to have multiple outputs</li>
</ul>

<p>Hyperparams:</p>
<ul>
  <li>Temperature:¬†Kept temps closer to 0 for most structural things (jsonifaction and stuff) while mainatained 0.3-0.5 for analysis to find a balance between creativity and structure</li>
</ul>

<h3 id="decision-3-separate-call-for-prompt-of-image-generation">Decision 3: Separate Call for prompt of image generation</h3>

<p>Initial approach: Earlier I was passing the analysis directly to the nano banana generation prompt but the output wasn‚Äôt that good. So I separately made llm call to generate prompt</p>

<p>Why I split them:  My intuition was that nano banana is good at generating or editing images given it is given clear cut instructions. We shouldn‚Äôt depend on it to reason and then generate.</p>

<p>Nano banana prompting technique:  For the prompt generation passed the <a href="https://ai.google.dev/gemini-api/docs/image-generation#best-practices" target="_blank" rel="noopener noreferrer">Nano Banana Best Practices as context</a></p>

<p>Results: More precise, actionable edits.</p>
<ul>
  <li>The edits were more subtle but comprehensible</li>
</ul>

<h3 id="decision-4-what-not-to-do-negative-prompting">Decision 4: What NOT to Do. Negative prompting</h3>

<ul>
  <li>Don‚Äôt add objects that weren‚Äôt there (authenticity matters)</li>
  <li>Keep text digestible - no one reads walls of text</li>
  <li>Focus on enhancement, not transformation</li>
</ul>

<h3 id="trade-offs-cost-vs-quality">Trade-offs (cost vs. quality):</h3>
<ul>
  <li>As of now, I haven‚Äôt though much about cost.</li>
  <li>Made as many llm calls.</li>
  <li>Idea is to get best results and then optimise for cost.</li>
  <li>Some ideas involve
    <ul>
      <li>Not making llm calls for json - instead use markdown parsers</li>
      <li>Some changes can be made using python tools instead of image generation</li>
      <li>Later: Finetune some opensource model and use it</li>
    </ul>
  </li>
</ul>

<h3 id="the-metric-saga">The Metric Saga</h3>

<p>Added metrics tracking. Then removed it.</p>

<p>Why?</p>
<ul>
  <li>Had tried to give some quantitative insights. Used LLMs to search for metrics like sharpness, color compositions.</li>
</ul>

<p>What I learned: I learned that it made no sense as image generated is completely new. So metrics might be coming as good but would made no sense.</p>

<div class="image-grid">
  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/og_image.png" alt="Original photo" />
    <figcaption>Original</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var1.png" alt="Enhanced variation 1" />
    <figcaption>Variation 1</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var2.png" alt="Enhanced variation 2" />
    <figcaption>Variation 2</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var3.png" alt="Enhanced variation 3" />
    <figcaption>Variation 3</figcaption>
  </figure>
</div>
<hr />

<h2 id="technical-challenges--solutions">Technical Challenges &amp; Solutions</h2>

<p><strong>Challenge 1</strong>: MIME type detection for various image formats
[Your solution]</p>

<p><strong>Challenge 2</strong>: Handling large images efficiently
[Your solution]</p>

<p><strong>Challenge 3</strong>: Balancing LLM call costs with quality
[Your solution]</p>

<p><strong>Challenge 4</strong>: Making AI feedback actually useful (not generic)
[Your solution]</p>

<p>[Optional: Include code snippet or flowchart]</p>

<hr />

<h2 id="what-i-built-vs-what-i-learned">What I Built vs. What I Learned</h2>

<p><strong>The product</strong>: A working AI photography assistant.</p>

<p><strong>The real wins:</strong></p>
<ul>
  <li>Understanding LLM prompting nuances in terms of image generation</li>
  <li>Caching strategies that actually matter</li>
  <li>Iteration over perfection</li>
  <li>Knowing when to remove features</li>
</ul>

<p>[Your personal reflections]</p>

<hr />

<h2 id="whats-next">What‚Äôs Next</h2>

<p>Potential improvements:</p>
<ul>
  <li>Batch processing</li>
  <li>Style preferences/learning</li>
  <li>Mobile app integration</li>
  <li>Community sharing features</li>
</ul>

<p>Open questions:</p>
<ul>
  <li>How to balance automation with creative control?</li>
  <li>What makes AI feedback feel ‚Äúauthentic‚Äù vs. generic?</li>
</ul>

<hr />

<h2 id="conclusion-the-side-project-effect">Conclusion: The Side Project Effect</h2>

<p>Started wanting to improve my photos. Ended up learning system design, LLM engineering, and product iteration.</p>

<p>Frame AI isn‚Äôt just a tool - it‚Äôs a learning artifact.</p>

<p>[Your closing thoughts]</p>

<p><strong>Try it, break it, let me know what you think</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a></p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/hero-app.png" alt="Frame AI in action" />
  <figcaption>Frame AI in action</figcaption>
</figure>

    </div>

    
    <div style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e2e8f0; font-size: 0.9rem; color: #666; text-align: center; font-style: italic;">
        Built with curiosity, debugged with patience, polished with <span style='color: #3182ce; font-weight: 500;'>Claude</span>.
    </div>
    
</article>

<div style="text-align: center; margin-top: 2rem;">
    <p style="color: #666;">Thanks for reading! ‚ú®</p>
</div>
      </main>

      <footer class="site-footer" role="contentinfo">
  <div class="social-icons">
    <a href="mailto:bejay.ketan1@gmail.com" class="social-icon" aria-label="Email">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.89 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
      </svg>
    </a>
    <a href="https://github.com/BKG123" target="_blank" class="social-icon" aria-label="GitHub">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M12 0C5.374 0 0 5.373 0 12 0 17.302 3.438 21.8 8.207 23.387c.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/>
      </svg>
    </a>
    <a href="https://www.linkedin.com/in/bejay-ketan-guin-67970018a" target="_blank" class="social-icon" aria-label="LinkedIn">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
      </svg>
    </a>
    <a href="https://x.com/bkguin" target="_blank" class="social-icon" aria-label="Twitter">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
      </svg>
    </a>
  </div>
  <p class="footer-text">Best way to reach me is to drop a note to my gmail.</p>
</footer>
    </div>

    <script src="/assets/js/theme.js"></script>
  </body>
</html>