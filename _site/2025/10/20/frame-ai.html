<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Frame AI: Building an AI-Powered Photography Assistant | Bejay‚Äôs Website</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Frame AI: Building an AI-Powered Photography Assistant" />
<meta name="author" content="Bejay" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I built an AI system that analyzes and enhances photos while teaching me." />
<meta property="og:description" content="How I built an AI system that analyzes and enhances photos while teaching me." />
<link rel="canonical" href="http://localhost:4000/2025/10/20/frame-ai.html" />
<meta property="og:url" content="http://localhost:4000/2025/10/20/frame-ai.html" />
<meta property="og:site_name" content="Bejay‚Äôs Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-20T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Frame AI: Building an AI-Powered Photography Assistant" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Bejay"},"dateModified":"2025-10-20T00:00:00+05:30","datePublished":"2025-10-20T00:00:00+05:30","description":"How I built an AI system that analyzes and enhances photos while teaching me.","headline":"Frame AI: Building an AI-Powered Photography Assistant","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/10/20/frame-ai.html"},"url":"http://localhost:4000/2025/10/20/frame-ai.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=00fd59ab2ac567ff5b5ad12f0035c337f0ced548">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  </head>
  <body>
    <div class="scroll-progress"></div>
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <div class="container">
      <header class="site-header" role="banner">
  <nav class="top-nav" role="navigation" aria-label="Main navigation">
    <a href="/" class="nav-link">about</a>
    <a href="/blog/" class="nav-link">blog</a>
    <a href="/projects/" class="nav-link">projects</a>
    <div class="nav-icons">
      <span class="theme-icon">üåô</span>
    </div>
  </nav>
</header>

      <main id="main-content" class="main-content" role="main">
        <div style="text-align: center; margin-bottom: 2rem;">
    <a href="/blog/" style="color: #3182ce; text-decoration: none; font-weight: 500;">‚Üê Back to Blog</a>
</div>

<article class="content-section">
    <header class="post-header">
        <h1 class="post-title">Frame AI: Building an AI-Powered Photography Assistant</h1>
        <p class="post-meta">
            <time datetime="2025-10-20T00:00:00+05:30">October 20, 2025</time>
             ‚Ä¢ by Bejay
        </p>
    </header>

    <div class="post-content">
        <h2 id="introduction-the-mobile-photography-itch">Introduction: The Mobile Photography Itch</h2>

<p>Every developer will tell you this: ‚ÄúI want to work on a side project to improve my portfolio.‚Äù But almost all of them will also admit they never got around to building it. I wanted to break that loop, so I started hunting for ideas. I‚Äôve always been fascinated by photography. Never professional-level good, but I can click half-decent pics on my iPhone. One of my friends introduced me to amateur photography principles ‚Äî rule of thirds, leading lines, proper lighting. I try to keep those in mind while taking snaps. More often than not, I fail, lol. That‚Äôs when the idea hit me: What if I could analyze images using vision LLMs (like Gemini 2.5 Flash) by prompting them correctly to check alignment with widely accepted photography rules? As I was iterating on the project, Google launched Gemini 2.5 Flash Image (nicknamed ‚Äúnano-banana‚Äù by the developer community) in August 2025 ‚Äî a breakthrough in image generation and editing that hit #1 on LMArena‚Äôs leaderboards. I thought, why not edit the images based on the analysis? So yeah, in short: Frame AI analyzes images and critiques them, and you can enhance images using Gemini 2.5 Flash Image.</p>

<p><strong>Why build Frame AI as a side project:</strong></p>
<ul>
  <li>Bridge the gap between taking photos and knowing how to improve them</li>
  <li>Explore the AI + photography intersection</li>
  <li>Build a real learning playground for system design</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/app-1.png" alt="Sample app image" />
  <figcaption>Frame AI Home</figcaption>
</figure>

<hr />

<h2 id="what-frame-ai-actually-does">What Frame AI Actually Does</h2>

<p><strong>The core idea</strong>: An AI assistant that understands photography principles.</p>

<p>Not just ‚Äúmake it prettier‚Äù - actual compositional feedback. There are fixed rules in photography: rule of thirds, leading lines, lighting, balance, and so on. Frame AI analyzes against these principles and suggests improvements.</p>

<p><strong>Two main features:</strong></p>

<ol>
  <li><strong>Analysis</strong>: What‚Äôs working, what‚Äôs not, and why</li>
  <li><strong>Enhancement</strong>: AI-powered edits based on best practices</li>
</ol>

<p><strong>The interesting twist</strong>: Instructions generated separately -&gt; So basically I make a separate call to the LLM to generate 3 distinct editing prompts from the analysis, each focusing on different aspects: technical perfection, atmospheric reinterpretation, and conceptual narrative. It also has access to the best practices of Gemini 2.5 Flash Image prompting techniques.</p>

<p>Earlier I was passing the analysis directly to the image generation prompt but the output wasn‚Äôt that good. My intuition was that Gemini 2.5 Flash Image excels at generating or editing images when given clear, specific instructions. We shouldn‚Äôt depend on it to reason and then generate ‚Äî separation of concerns works better.</p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/quick-analysis.png" alt="Sample analysis output" />
  <figcaption>Digestible, bullet-pointed feedback that actually helps</figcaption>
</figure>

<hr />

<h2 id="system-design-how-it-all-fits-together">System Design: How It All Fits Together</h2>

<p><strong>High-level architecture:</strong></p>

<p>User uploads image ‚Üí FastAPI backend ‚Üí Image processing &amp; caching layer ‚Üí LLM analysis ‚Üí Stored in DB ‚Üí Enhance Image Trigger ‚Üí 3 editing prompts generated from analysis ‚Üí 3 images generated in parallel using Gemini 2.5 Flash Image</p>

<p><strong>Key components:</strong></p>

<ul>
  <li><strong>Database</strong>: SQLite (content-based hash indexing for deduplication)</li>
  <li><strong>LLM Integration</strong>: Gemini 2.5 Flash for analysis, Gemini 2.5 Flash Lite for JSON structuring, Gemini 2.5 Flash Image for enhancement</li>
  <li><strong>Caching Strategy</strong>: Content-based hashing prevents duplicate processing (performance + cost optimization)</li>
  <li><strong>API Design</strong>: RESTful endpoints with Server-Sent Events for streaming analysis</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/system-design.png" alt="System architecture diagram" data-lightbox="image" />
  <figcaption>Clean architectural overview of Frame AI</figcaption>
</figure>

<p>As mentioned earlier, the product has two main parts: image analysis and image enhancement.</p>

<p><strong>Image Analysis Flow:</strong></p>
<ol>
  <li>Frontend calls <code class="language-plaintext highlighter-rouge">/upload</code> API with image file</li>
  <li>Backend workflow:
    <ul>
      <li>Content-based hash is generated from file bytes (fixes duplicate detection edge cases)</li>
      <li>Image stored permanently in <code class="language-plaintext highlighter-rouge">static/uploaded_images/{hash}.{ext}</code> (deduplication happens here)</li>
      <li>Check SQLite cache ‚Äî if hash exists, stream cached analysis immediately</li>
      <li>For new images:
        <ul>
          <li>Temporary file created for processing</li>
          <li>Gemini 2.5 Flash call with image + photography best practices prompt</li>
          <li>LLM streams detailed analysis with scores (exposure, composition, lighting, overall)</li>
          <li>Analysis stored in SQLite against file hash</li>
          <li>Frontend receives analysis via Server-Sent Events (real-time streaming)</li>
        </ul>
      </li>
      <li>EXIF data extracted and stored for context</li>
    </ul>
  </li>
</ol>

<p><strong>Image Enhancement Flow:</strong></p>
<ol>
  <li>Frontend calls <code class="language-plaintext highlighter-rouge">/image/edit</code> API with file hash</li>
  <li>Backend workflow:
    <ul>
      <li>Fetch cached analysis from SQLite using hash</li>
      <li>Generate 3 distinct editing prompts via Gemini 2.5 Flash (with Gemini 2.5 Flash Image best practices as context)
        <ul>
          <li>Prompt 1: Technical perfection &amp; enhancement</li>
          <li>Prompt 2: Atmospheric &amp; mood reinterpretation</li>
          <li>Prompt 3: Conceptual &amp; narrative composite</li>
        </ul>
      </li>
      <li>Launch 3 parallel Gemini 2.5 Flash Image API calls (<code class="language-plaintext highlighter-rouge">asyncio.gather</code>)</li>
      <li>Each call returns: enhanced image + text description of changes</li>
      <li>Text descriptions converted to structured JSON via Gemini 2.5 Flash Lite (temperature = 0 for consistency)</li>
      <li>Return 3 enhanced images with metadata to frontend</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="design-decisions--lessons-learned">Design Decisions &amp; Lessons Learned</h2>

<h3 id="decision-1-content-based-hashing-for-caching">Decision 1: Content-Based Hashing for Caching</h3>

<p><strong>Why it‚Äôs critical:</strong></p>
<ul>
  <li>Analysis needs to be reused for enhancement (same image = same analysis)</li>
  <li>Duplicate uploads waste API calls and cost money</li>
  <li>User experience: instant results for previously analyzed images</li>
</ul>

<p><strong>Evolution of approach:</strong></p>
<ul>
  <li><strong>First attempt</strong>: Filename + IP-based caching
    <ul>
      <li>Problem: Same filename, different image = cache collision</li>
      <li>Problem: Storing IP addresses = PII concerns</li>
    </ul>
  </li>
  <li><strong>Final solution</strong>: Content-based hashing (SHA-256 of file bytes)
    <ul>
      <li>Same image content ‚Üí same hash ‚Üí reliable cache hit</li>
      <li>Different images ‚Üí different hashes ‚Üí no false positives</li>
      <li>Privacy-friendly: no PII stored</li>
    </ul>
  </li>
</ul>

<p><strong>Impact</strong>: ~40% cache hit rate in production (multiple users upload popular stock photos for testing)</p>

<h3 id="decision-2-three-parallel-image-variations">Decision 2: Three Parallel Image Variations</h3>

<p><strong>Why not just one enhanced image?</strong></p>
<ul>
  <li>Photography enhancement is highly subjective</li>
  <li>Analysis covers multiple dimensions (technical, artistic, mood)</li>
  <li>Users prefer choice ‚Äî what looks ‚Äúbetter‚Äù varies by taste and use case</li>
  <li>Three variations let users see different creative directions</li>
</ul>

<p><strong>The three approaches:</strong></p>
<ol>
  <li><strong>Technical perfection</strong>: Fix exposure, sharpen details, recover dynamic range</li>
  <li><strong>Atmospheric reinterpretation</strong>: Transform mood through color grading and lighting</li>
  <li><strong>Conceptual narrative</strong>: Reimagine the story (subtle compositing, creative edits)</li>
</ol>

<p><strong>Hyperparameter tuning:</strong></p>
<ul>
  <li><strong>Analysis LLM</strong> (Gemini 2.5 Flash): temperature = 0.3-0.5 (balance creativity with structure)</li>
  <li><strong>JSON structuring</strong> (Gemini 2.5 Flash Lite): temperature = 0 (deterministic output)</li>
  <li><strong>Prompt generation</strong> (Gemini 2.5 Flash): temperature = 0.5 (creative but focused)</li>
</ul>

<p><strong>Performance</strong>: Using <code class="language-plaintext highlighter-rouge">asyncio.gather()</code> for parallel generation reduces total wait time from ~45s to ~15s</p>

<h3 id="decision-3-separate-llm-call-for-prompt-generation">Decision 3: Separate LLM Call for Prompt Generation</h3>

<p><strong>Initial approach</strong>: Passing analysis directly to Gemini 2.5 Flash Image</p>
<ul>
  <li>Problem: Output quality was inconsistent</li>
  <li>Image model struggled to extract actionable edits from verbose analysis</li>
</ul>

<p><strong>Final solution</strong>: Dedicated prompt generation step</p>
<ul>
  <li>Generate 3 specific editing prompts via Gemini 2.5 Flash</li>
  <li>Include <a href="https://ai.google.dev/gemini-api/docs/image-generation#best-practices" target="_blank" rel="noopener noreferrer">Gemini 2.5 Flash Image best practices</a> as context</li>
  <li>Pass clean, focused instructions to image model</li>
</ul>

<p><strong>Why it works</strong>: Gemini 2.5 Flash Image excels at following clear, step-by-step instructions ‚Äî but we shouldn‚Äôt ask it to reason about photography theory <em>and</em> generate images. Separation of concerns wins again.</p>

<p><strong>Results</strong>: More precise, actionable edits. The enhancements became subtle but comprehensible instead of over-processed.</p>

<h3 id="decision-4-design-constraints--negative-prompting">Decision 4: Design Constraints &amp; Negative Prompting</h3>

<p><strong>What I explicitly told the models NOT to do:</strong></p>
<ul>
  <li>Don‚Äôt add objects/people that weren‚Äôt in the original (authenticity matters for photography)</li>
  <li>Don‚Äôt rotate or change orientation (preserve photographer‚Äôs intent)</li>
  <li>Don‚Äôt over-process to the point of looking fake</li>
</ul>

<p><strong>UX decisions:</strong></p>
<ul>
  <li>Keep feedback digestible ‚Äî no one reads walls of text</li>
  <li>Focus on enhancement, not transformation</li>
  <li>Make AI feedback feel specific, not generic</li>
</ul>

<h3 id="trade-offs-cost-vs-quality">Trade-offs: Cost vs. Quality</h3>

<p><strong>Current approach</strong>: Optimize for quality first</p>
<ul>
  <li>Making multiple LLM calls per request (analysis + prompt generation + JSON structuring)</li>
  <li>Not worrying about cost during initial development</li>
  <li>Philosophy: Get best results, then optimize</li>
</ul>

<p><strong>Future optimizations:</strong></p>
<ul>
  <li>Replace JSON-generating LLM calls with markdown parsers</li>
  <li>Use Python imaging libraries (Pillow) for simple adjustments instead of image generation</li>
  <li>Fine-tune an open-source model for analysis</li>
  <li>Batch processing for multiple images</li>
</ul>

<h3 id="the-metric-saga">The Metric Saga</h3>

<p><strong>What happened</strong>: Added metrics tracking. Then removed it.</p>

<p><strong>Why?</strong></p>
<ul>
  <li>Had tried to give some quantitative insights using LLMs to calculate metrics like sharpness, color composition</li>
  <li>Problem: The generated image is completely new, not a modified version of the original</li>
  <li>Metrics might show ‚Äúimprovement‚Äù but they‚Äôre comparing apples to oranges</li>
</ul>

<p><strong>What I learned</strong>: For AI-generated image enhancements, subjective visual comparison beats objective metrics. The user‚Äôs eye is the best judge.</p>

<div class="image-grid">
  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/og_image.png" alt="Original photo" />
    <figcaption>Original</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var1.png" alt="Enhanced variation 1" />
    <figcaption>Variation 1</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var2.png" alt="Enhanced variation 2" />
    <figcaption>Variation 2</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var3.png" alt="Enhanced variation 3" />
    <figcaption>Variation 3</figcaption>
  </figure>
</div>

<hr />

<h2 id="technical-challenges--solutions">Technical Challenges &amp; Solutions</h2>

<h3 id="challenge-1-mime-type-detection-for-various-image-formats">Challenge 1: MIME Type Detection for Various Image Formats</h3>

<p><strong>Problem</strong>: Different browsers and clients send images with different MIME types. Some don‚Äôt include proper Content-Type headers.</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Implemented a helper function <code class="language-plaintext highlighter-rouge">get_file_mime_type()</code> that detects MIME type from:
    <ol>
      <li>File extension (<code class="language-plaintext highlighter-rouge">.jpg</code>, <code class="language-plaintext highlighter-rouge">.png</code>, <code class="language-plaintext highlighter-rouge">.heic</code>, etc.)</li>
      <li>HTTP headers when fetching remote images</li>
      <li>Fallback to <code class="language-plaintext highlighter-rouge">image/jpeg</code> for unknown types</li>
    </ol>
  </li>
  <li>Used Python‚Äôs <code class="language-plaintext highlighter-rouge">mimetypes</code> library for reliable extension-to-MIME mapping</li>
</ul>

<h3 id="challenge-2-handling-large-images--exif-orientation">Challenge 2: Handling Large Images &amp; EXIF Orientation</h3>

<p><strong>Problem</strong>:</p>
<ul>
  <li>Large images (&gt;10MB) were slow to process</li>
  <li>iPhone photos often had incorrect orientation due to EXIF rotation metadata</li>
</ul>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Used Pillow‚Äôs <code class="language-plaintext highlighter-rouge">ImageOps.exif_transpose()</code> to auto-correct orientation before processing</li>
  <li>Temporary files created with proper cleanup using context managers</li>
  <li>Server-Sent Events for streaming analysis so users see progress immediately</li>
</ul>

<p><strong>Code snippet</strong> (<a href="utils/helpers.py#L1-L20">utils/helpers.py:1-20</a>):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_file_mime_type</span><span class="p">(</span><span class="n">file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="s">"""Detect MIME type from file extension or headers"""</span>
    <span class="n">mime_type</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">mimetypes</span><span class="p">.</span><span class="n">guess_type</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mime_type</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mime_type</span>
    <span class="c1"># Fallback for common extensions
</span>    <span class="k">if</span> <span class="n">file_path</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">'.heic'</span><span class="p">):</span>
        <span class="k">return</span> <span class="s">'image/heic'</span>
    <span class="k">return</span> <span class="s">'image/jpeg'</span>
</code></pre></div></div>

<h3 id="challenge-3-balancing-llm-call-costs-with-quality">Challenge 3: Balancing LLM Call Costs with Quality</h3>

<p><strong>Problem</strong>: Each image analysis + enhancement requires 6+ LLM calls. Costs add up quickly.</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Implemented aggressive caching with content-based hashing (40% hit rate)</li>
  <li>Used cheaper models where possible (Gemini 2.5 Flash Lite for JSON structuring)</li>
  <li>Set temperature = 0 for deterministic tasks to avoid retries</li>
  <li>Future: Will implement batch processing and consolidate LLM calls</li>
</ul>

<p><strong>Cost breakdown</strong> (per image):</p>
<ul>
  <li>Analysis: ~$0.01 (Gemini 2.5 Flash with vision)</li>
  <li>Prompt generation: ~$0.002 (Gemini 2.5 Flash)</li>
  <li>3x Image generation: ~$0.12 ($0.039 per image)</li>
  <li>3x JSON structuring: ~$0.003 (Gemini 2.5 Flash Lite)</li>
  <li><strong>Total: ~$0.135 per full analysis + enhancement</strong></li>
</ul>

<h3 id="challenge-4-making-ai-feedback-actually-useful-not-generic">Challenge 4: Making AI Feedback Actually Useful (Not Generic)</h3>

<p><strong>Problem</strong>: Early versions gave generic feedback like ‚Äúnice composition‚Äù or ‚Äúgood lighting‚Äù that didn‚Äôt help users improve.</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Crafted a detailed system prompt with specific photography principles (rule of thirds, leading lines, etc.)</li>
  <li>Included EXIF data in the prompt so the LLM could explain technical settings</li>
  <li>Asked for structured feedback: strengths, improvements, professional tips</li>
  <li>Added numerical scores (1-10) for exposure, composition, lighting, overall</li>
</ul>

<p><strong>Example of specific vs. generic feedback</strong>:</p>
<ul>
  <li>‚ùå Generic: ‚ÄúThe lighting could be better‚Äù</li>
  <li>‚úÖ Specific: ‚ÄúThe main light source creates harsh shadows on the subject‚Äôs face. Try shooting during the ‚Äògolden hour‚Äô or using a diffuser to soften the light.‚Äù</li>
</ul>

<hr />

<h2 id="what-i-built-vs-what-i-learned">What I Built vs. What I Learned</h2>

<p><strong>The product</strong>: A working AI photography assistant that analyzes and enhances photos in real-time.</p>

<p><strong>The real wins:</strong></p>

<p>After 3 years working with LLMs, this project taught me lessons you can‚Äôt learn from enterprise work alone:</p>

<ul>
  <li>
    <p><strong>Image models are fundamentally different from text models</strong>: Thought my LLM prompting experience would transfer directly. It didn‚Äôt. Image generation models need surgical precision in instructions, not reasoning ability. Separation of concerns (reasoning LLM ‚Üí instruction generation ‚Üí image model) worked better than end-to-end prompting.</p>
  </li>
  <li>
    <p><strong>Real-world system design beats architectural purity</strong>: Content-based hashing wasn‚Äôt the ‚Äúcleanest‚Äù solution, but it solved real problems (cache collisions, PII concerns). Sometimes the pragmatic choice is the right choice.</p>
  </li>
  <li>
    <p><strong>Caching is both UX and economics</strong>: 40% cache hit rate doesn‚Äôt just save money ‚Äî it makes the product feel responsive. Users don‚Äôt care about Server-Sent Events or streaming; they care that analysis feels instant on the second try.</p>
  </li>
  <li>
    <p><strong>Iteration beats perfection</strong>: Shipped with basic features, then improved based on real usage. The metrics tracking feature sounded great on paper, but added zero value in practice. Removing it improved focus.</p>
  </li>
  <li>
    <p><strong>Users want agency, not magic</strong>: Three variations¬†¬ª&gt; one ‚Äúperfect‚Äù edit. For subjective tasks like photography, showing the AI‚Äôs work (via text descriptions) builds more trust than hiding complexity.</p>
  </li>
  <li>
    <p><strong>Async architecture compounds gains</strong>: <code class="language-plaintext highlighter-rouge">asyncio.gather()</code> for parallel image generation wasn‚Äôt just about speed (3x faster) ‚Äî it fundamentally changed the user experience from ‚Äúgo grab coffee‚Äù to ‚Äúwait a moment.‚Äù</p>
  </li>
</ul>

<h3 id="personal-reflections">Personal Reflections</h3>

<p>Building Frame AI taught me something important: <strong>side projects are learning artifacts</strong>. After 3 years in the LLM industry, I thought I knew this space. But there‚Äôs a difference between integrating LLMs into existing products and building something from scratch. The messy middle ‚Äî debugging EXIF orientation at 2am, discovering my caching strategy had edge cases, iterating on prompts 20+ times ‚Äî that‚Äôs where real learning happens.</p>

<p>Frame AI isn‚Äôt perfect, and it probably never will be. But it‚Äôs a snapshot of what I know <em>right now</em> about LLMs, system design, and product thinking. And that‚Äôs valuable.</p>

<hr />

<h2 id="whats-next">What‚Äôs Next</h2>

<p><strong>Potential improvements:</strong></p>
<ul>
  <li><strong>Batch processing</strong>: Upload multiple photos, get bulk analysis</li>
  <li><strong>Style preferences/learning</strong>: Remember user preferences (prefers moody edits vs. bright and airy)</li>
  <li><strong>Mobile app integration</strong>: PWA or native iOS app for on-the-go analysis</li>
  <li><strong>Community sharing features</strong>: Gallery of before/after examples, upvoting best enhancements</li>
  <li><strong>Cost optimization</strong>: Consolidate LLM calls, use open-source models for some tasks</li>
</ul>

<p><strong>Open questions I‚Äôm still exploring:</strong></p>
<ul>
  <li>How to balance automation with creative control? (Let users tweak prompts? Provide sliders for ‚Äúenhancement intensity‚Äù?)</li>
  <li>What makes AI feedback feel ‚Äúauthentic‚Äù vs. generic? (Still experimenting with prompt engineering)</li>
  <li>Should I add a ‚Äúlearn from feedback‚Äù loop where users mark helpful vs. unhelpful critiques?</li>
</ul>

<hr />

<h2 id="conclusion-the-side-project-effect">Conclusion: The Side Project Effect</h2>

<p>Started wanting to improve my photos. Ended up learning system design, LLM engineering, and product iteration.</p>

<p>Frame AI isn‚Äôt just a tool ‚Äî it‚Äôs a learning artifact. It represents 2 weeks of curiosity-driven exploration, late-night debugging, and iterative improvement. I learned more from building this than I would have from a dozen tutorials.</p>

<p>If you‚Äôre a developer thinking ‚ÄúI should build a side project,‚Äù stop thinking and start building. Pick something you‚Äôre genuinely curious about, not what‚Äôs trending on Twitter. Your first version will be messy. Your caching strategy will have edge cases. Your prompts will need 20 iterations. That‚Äôs fine. That‚Äôs the point.</p>

<p>The real value isn‚Äôt the final product ‚Äî it‚Äôs the process of figuring things out.</p>

<p><strong>Try it, break it, let me know what you think</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a></p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/hero-app.png" alt="Frame AI in action" />
  <figcaption>Frame AI in action</figcaption>
</figure>

    </div>

    
    <div style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e2e8f0; font-size: 0.9rem; color: #666; text-align: center; font-style: italic;">
        Built with curiosity, debugged with patience, polished with <span style='color: #3182ce; font-weight: 500;'>Claude</span>.
    </div>
    
</article>

<div style="text-align: center; margin-top: 2rem;">
    <p style="color: #666;">Thanks for reading! ‚ú®</p>
</div>
      </main>

      <footer class="site-footer" role="contentinfo">
  <div class="social-icons">
    <a href="mailto:bejay.ketan1@gmail.com" class="social-icon" aria-label="Email">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.89 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
      </svg>
    </a>
    <a href="https://github.com/BKG123" target="_blank" class="social-icon" aria-label="GitHub">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M12 0C5.374 0 0 5.373 0 12 0 17.302 3.438 21.8 8.207 23.387c.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/>
      </svg>
    </a>
    <a href="https://www.linkedin.com/in/bejay-ketan-guin-67970018a" target="_blank" class="social-icon" aria-label="LinkedIn">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
      </svg>
    </a>
    <a href="https://x.com/bkguin" target="_blank" class="social-icon" aria-label="Twitter">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
        <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
      </svg>
    </a>
  </div>
  <p class="footer-text">Best way to reach me is to drop a note to my gmail.</p>
</footer>
    </div>

    <script src="/assets/js/theme.js"></script>
  </body>
</html>