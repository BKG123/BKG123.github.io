<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-20T23:24:00+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Bejay’s Website</title><subtitle>Welcome to my little corner of the internet. Full-time engineer passionate about GenAI, building magical experiences with clarity and logic.</subtitle><entry><title type="html">Frame AI: Building an AI-Powered Photography Assistant</title><link href="http://localhost:4000/2025/10/20/frame-ai.html" rel="alternate" type="text/html" title="Frame AI: Building an AI-Powered Photography Assistant" /><published>2025-10-20T00:00:00+05:30</published><updated>2025-10-20T00:00:00+05:30</updated><id>http://localhost:4000/2025/10/20/frame-ai</id><content type="html" xml:base="http://localhost:4000/2025/10/20/frame-ai.html"><![CDATA[<h2 id="introduction-the-mobile-photography-itch">Introduction: The Mobile Photography Itch</h2>

<p>Every developer will tell you this: “I want to work on a side project to improve my portfolio.” But almost all of them will also admit they never got around to building it. I wanted to break that loop, so I started hunting for ideas. I’ve always been fascinated by photography. Never professional-level good, but I can click half-decent pics on my iPhone. One of my friends introduced me to amateur photography principles — rule of thirds, leading lines, proper lighting. I try to keep those in mind while taking snaps. More often than not, I fail, lol. That’s when the idea hit me: What if I could analyze images using vision LLMs (like gemini-2.5-flash) by prompting them correctly to check alignment with widely accepted photography rules? As I was iterating on the project, Google stealthily launched nano-banana — a revolution in closed-model image generation. I thought, why not edit the images based on the analysis? So yeah, in short: Frame AI analyzes images and critiques them, and you can enhance images using nano-banana.</p>

<p><strong>Why build Frame AI as a side project:</strong></p>
<ul>
  <li>Bridge the gap between taking photos and knowing how to improve them</li>
  <li>Explore the AI + photography intersection</li>
  <li>Build a real learning playground for system design</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/app-1.png" alt="Sample app image" />
  <figcaption>Frame AI Home</figcaption>
</figure>

<hr />

<h2 id="what-frame-ai-actually-does">What Frame AI Actually Does</h2>

<p><strong>The core idea</strong>: An AI assistant that understands photography principles.</p>

<p>Not just “make it prettier” - actual compositional feedback. There are fixed rules in photography: rule of thirds, leading lines, lighting, balance, and so on. Frame AI analyzes against these principles and suggests improvements.</p>

<p><strong>Two main features:</strong></p>

<ol>
  <li><strong>Analysis</strong>: What’s working, what’s not, and why</li>
  <li><strong>Enhancement</strong>: AI-powered edits based on best practices</li>
</ol>

<p><strong>The interesting twist</strong>: Instructions generated separately -&gt; So basically I make a separate call to LLM to generate 3 prompts from the analysis, each focusing on separate parts of the feedback. It also has access to the best practices of nano banana prompting techniques.</p>

<p>Earlier I was passing the analysis directly to the nano banana generation prompt but the output wasn’t that good. My intuition was that nano banana is good at generating or editing images given it is given clear cut instructions. We shouldn’t depend on it to reason and then generate.</p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/quick-analysis.png" alt="Sample analysis output" />
  <figcaption>Digestible, bullet-pointed feedback that actually helps</figcaption>
</figure>

<hr />

<h2 id="system-design-how-it-all-fits-together">System Design: How It All Fits Together</h2>

<p><strong>High-level architecture:</strong></p>

<p>User uploads image → FastAPI backend → Image processing &amp; caching layer → LLM analysis → Stored in DB -&gt; Enhance Image Trigger → 3 prompts generated for analysis → 3 Images generated using nano-banana</p>

<p><strong>Key components:</strong></p>

<ul>
  <li><strong>Database</strong>: Sqlite</li>
  <li><strong>LLM Integration</strong>: Using Gemini Flash Models (“gemini-2.5-flash”, “gemini-2.5-flash-lite”, “gemini-2.5-flash-image”)</li>
  <li><strong>Caching Strategy</strong>: Why images being cached matters (performance + cost)</li>
  <li><strong>API Design</strong>: RESTful endpoints for upload, analyze, edit</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/system-design.png" alt="System architecture diagram" data-lightbox="image" />
  <figcaption>Clean architectural overview of Frame AI</figcaption>
</figure>

<p>As mentioned earlier the product has two main parts - image analysis and image enhancements.</p>

<p>Image Analysis:</p>
<ul>
  <li>
    <p>FE calls the upload api with image file</p>
  </li>
  <li>
    <p>In BE</p>
    <ul>
      <li>A hash is created using the contents of the file.</li>
      <li>The image file is stored in memory with the hash as the filename.</li>
      <li>A temporary file is created for analysis.</li>
      <li>A LLM call to gemini is made (“gemini-2.5-flash”) with the image as input and best practices of photography in the prompt</li>
      <li>The LLM outputs a detailed analysis along with scores.</li>
      <li>This “Detailed Analysis” is parsed and shown in the frontend.</li>
      <li>Another LLM call is made to output a “Quick Analysis” which is shown in a diff tab</li>
      <li>Both the analyses are stored in SQLite db against the file hash and the value is returned to FE, along with the file hash</li>
    </ul>
  </li>
</ul>

<p>Image Enhancement</p>
<ul>
  <li>
    <p>FE calls the image enhancement API with file hash</p>
  </li>
  <li>
    <p>In BE</p>
    <ul>
      <li>The analysis is fetched using the file hash.</li>
      <li>From the analysis, 3 prompts are generated using LLM call (nano banana best practices are passed as context)</li>
      <li>Using the 3 prompts, 3 parallel calls are made to nano banana api to generate 3 images and corresponding text outputs saying what changes were made</li>
      <li>These text outputs are passed through 3 llm calls to structure the output into json</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="design-decisions--lessons-learned">Design Decisions &amp; Lessons Learned</h2>

<h3 id="decision-1-image-caching">Decision 1: Image Caching</h3>

<p>Why it’s critical:</p>
<ul>
  <li>Caching of analysis is mostly necessary as I wanted to reuse it for enhancement.</li>
</ul>

<p>How I implemented it:</p>
<ul>
  <li>Frist tried with only filename + ip based caching but it was not that robust for same filename different image cases. Also didn’t want to store PII</li>
</ul>

<h3 id="decision-2-three-separate-generations-for-analysis">Decision 2: Three Separate Generations for Analysis</h3>

<p>Why not just one?</p>
<ul>
  <li>Enhancement of image is highly subjective, so it’s better to have 3 different outputs instead of 1. Also the analysis focuses on multiple params and its better to have multiple outputs</li>
</ul>

<p>Hyperparams:</p>
<ul>
  <li>Temperature: Kept temps closer to 0 for most structural things (jsonifaction and stuff) while mainatained 0.3-0.5 for analysis to find a balance between creativity and structure</li>
</ul>

<h3 id="decision-3-separate-call-for-prompt-of-image-generation">Decision 3: Separate Call for prompt of image generation</h3>

<p>Initial approach: Earlier I was passing the analysis directly to the nano banana generation prompt but the output wasn’t that good. So I separately made llm call to generate prompt</p>

<p>Why I split them:  My intuition was that nano banana is good at generating or editing images given it is given clear cut instructions. We shouldn’t depend on it to reason and then generate.</p>

<p>Nano banana prompting technique:  For the prompt generation passed the <a href="https://ai.google.dev/gemini-api/docs/image-generation#best-practices" target="_blank" rel="noopener noreferrer">Nano Banana Best Practices as context</a></p>

<p>Results: More precise, actionable edits.</p>
<ul>
  <li>The edits were more subtle but comprehensible</li>
</ul>

<h3 id="decision-4-what-not-to-do-negative-prompting">Decision 4: What NOT to Do. Negative prompting</h3>

<ul>
  <li>Don’t add objects that weren’t there (authenticity matters)</li>
  <li>Keep text digestible - no one reads walls of text</li>
  <li>Focus on enhancement, not transformation</li>
</ul>

<h3 id="trade-offs-cost-vs-quality">Trade-offs (cost vs. quality):</h3>
<ul>
  <li>As of now, I haven’t though much about cost.</li>
  <li>Made as many llm calls.</li>
  <li>Idea is to get best results and then optimise for cost.</li>
  <li>Some ideas involve
    <ul>
      <li>Not making llm calls for json - instead use markdown parsers</li>
      <li>Some changes can be made using python tools instead of image generation</li>
      <li>Later: Finetune some opensource model and use it</li>
    </ul>
  </li>
</ul>

<h3 id="the-metric-saga">The Metric Saga</h3>

<p>Added metrics tracking. Then removed it.</p>

<p>Why?</p>
<ul>
  <li>Had tried to give some quantitative insights. Used LLMs to search for metrics like sharpness, color compositions.</li>
</ul>

<p>What I learned: I learned that it made no sense as image generated is completely new. So metrics might be coming as good but would made no sense.</p>

<div class="image-grid">
  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/og_image.png" alt="Original photo" />
    <figcaption>Original</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var1.png" alt="Enhanced variation 1" />
    <figcaption>Variation 1</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var2.png" alt="Enhanced variation 2" />
    <figcaption>Variation 2</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var3.png" alt="Enhanced variation 3" />
    <figcaption>Variation 3</figcaption>
  </figure>
</div>
<hr />

<h2 id="technical-challenges--solutions">Technical Challenges &amp; Solutions</h2>

<p><strong>Challenge 1</strong>: MIME type detection for various image formats
[Your solution]</p>

<p><strong>Challenge 2</strong>: Handling large images efficiently
[Your solution]</p>

<p><strong>Challenge 3</strong>: Balancing LLM call costs with quality
[Your solution]</p>

<p><strong>Challenge 4</strong>: Making AI feedback actually useful (not generic)
[Your solution]</p>

<p>[Optional: Include code snippet or flowchart]</p>

<hr />

<h2 id="what-i-built-vs-what-i-learned">What I Built vs. What I Learned</h2>

<p><strong>The product</strong>: A working AI photography assistant.</p>

<p><strong>The real wins:</strong></p>
<ul>
  <li>Understanding LLM prompting nuances in terms of image generation</li>
  <li>Caching strategies that actually matter</li>
  <li>Iteration over perfection</li>
  <li>Knowing when to remove features</li>
</ul>

<p>[Your personal reflections]</p>

<hr />

<h2 id="whats-next">What’s Next</h2>

<p>Potential improvements:</p>
<ul>
  <li>Batch processing</li>
  <li>Style preferences/learning</li>
  <li>Mobile app integration</li>
  <li>Community sharing features</li>
</ul>

<p>Open questions:</p>
<ul>
  <li>How to balance automation with creative control?</li>
  <li>What makes AI feedback feel “authentic” vs. generic?</li>
</ul>

<hr />

<h2 id="conclusion-the-side-project-effect">Conclusion: The Side Project Effect</h2>

<p>Started wanting to improve my photos. Ended up learning system design, LLM engineering, and product iteration.</p>

<p>Frame AI isn’t just a tool - it’s a learning artifact.</p>

<p>[Your closing thoughts]</p>

<p><strong>Try it, break it, let me know what you think</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a></p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/hero-app.png" alt="Frame AI in action" />
  <figcaption>Frame AI in action</figcaption>
</figure>]]></content><author><name>Bejay</name></author><summary type="html"><![CDATA[How I built an AI system that analyzes and enhances photos while teaching me.]]></summary></entry><entry><title type="html">Everything in Life is Linear Regression</title><link href="http://localhost:4000/2025/10/16/life-is-linear-regression.html" rel="alternate" type="text/html" title="Everything in Life is Linear Regression" /><published>2025-10-16T00:00:00+05:30</published><updated>2025-10-16T00:00:00+05:30</updated><id>http://localhost:4000/2025/10/16/life-is-linear-regression</id><content type="html" xml:base="http://localhost:4000/2025/10/16/life-is-linear-regression.html"><![CDATA[<figure>
  <img src="/assets/images/2025-10-16-life-is-linear-regression/image1.png" alt="Everything in Life is Linear Regression" />
  <figcaption>Life's complexities can be understood through the lens of weighted combinations</figcaption>
</figure>

<p>When I started learning ML, I was first introduced to Linear Regression. In short, it describes an algorithm where you can model a function using a linear expression:</p>

<p><strong>y = wx + c</strong></p>

<p>Largely similar to the equation of a straight line. Here, the value of <strong>y</strong> (dependent variable) changes with <strong>x</strong> (independent variable).</p>

<p>Now, if we extrapolate this to multiple independent variables:</p>

<p><strong>y = w₁x₁ + w₂x₂ + … + wₙxₙ + c</strong></p>

<p>In most use cases of linear regression, this is the case. An outcome or output is dependent on multiple factors.</p>

<p>Suppose you’re modeling the house price of a city using linear regression. You’ll find that historically, the price of a house depends on multiple factors — area, number of rooms, sq footage, parking (available or not), and so on. The Linear Regression algorithm tries to find those coefficients — w₁, w₂, … wₙ — and we get a model (or equation) on which, if we feed in new values of x₁…xₙ, we can “predict” or “estimate” the cost of the house in question.</p>

<p>Now, the idea of this blog is not to deep dive into LR. It’s because I seem to find a parallel between everything in life and this mathematical concept — not the linear part, but the <strong>combination part</strong> where everything is a combination of multiple things with different scaling factors associated with each of them.</p>

<hr />

<h3 id="for-example">For example:</h3>

<p>Suppose you missed a train on a certain day. You become extremely angry and start blaming your mom for apparently “making you late” by asking you to eat breakfast before leaving. But this is <strong>black-and-white thinking</strong> — sure, it might have played a role. But there are other factors here as well to consider. Like the fact that you slept late last night despite knowing you have a train to catch the next day. Also, the traffic at that time was more than usual.</p>

<figure>
  <img src="/assets/images/2025-10-16-life-is-linear-regression/image2.png" alt="Multiple factors contributing to missing a train" />
  <figcaption>Missing the train isn't about one factor — it's a weighted combination of breakfast delay, waking up late, traffic, and more</figcaption>
</figure>

<p>If I were to put it in the equation:</p>

<p><strong>minutes_late = w₁(breakfast_delay) + w₂(woke_up_late) + w₃(traffic_level) + w₄(distance_to_station) + w₅(train_punctuality) + c</strong></p>

<p>Where:</p>
<ul>
  <li><strong>minutes_late</strong> = how many minutes late you arrived at the station (or how close you were to missing the train)</li>
  <li><strong>breakfast_delay</strong> = time spent on breakfast (in minutes)</li>
  <li><strong>woke_up_late</strong> = how late you woke up compared to planned time (in minutes)</li>
  <li><strong>traffic_level</strong> = traffic congestion factor (could be 1-10 scale, or actual delay in minutes)</li>
  <li><strong>distance_to_station</strong> = distance you need to travel (in km)</li>
  <li><strong>train_punctuality</strong> = how early/late the train typically runs (in minutes)</li>
  <li><strong>c</strong> = baseline constant (accounts for other unmeasured factors)</li>
</ul>

<p>The weights (w₁, w₂, w₃, etc.) represent how much each factor contributes to the outcome. For instance:</p>
<ul>
  <li>Maybe <strong>w₂</strong> is large because waking up late has a huge cascading effect</li>
  <li><strong>w₁</strong> might be small because breakfast only added 5 minutes</li>
  <li><strong>w₃</strong> could be moderate depending on how unpredictable traffic is</li>
</ul>

<hr />

<p>The more experiences I have in life, the more I resonate with this.</p>

<p>Now, I know real linear regression has assumptions about linearity and independence that life often violates. But as a mental model for thinking about multiple factors contributing to outcomes, it works surprisingly well.</p>

<p>This also helps me approach differences in opinions in a calmer and composed manner. Let’s say India wins a cricket match — some say it was because of Virat’s ton. Some say it’s because of Bumrah’s fifer. Or some say it was because of Rohit’s quickfire 25 off 10 balls.</p>

<p>I say <strong>it’s all of that</strong>. Just with different weights.</p>

<hr />

<h3 id="another-example">Another example:</h3>

<p>Someone says, “They broke up because he was toxic.”</p>

<p>But reality?</p>

<p><strong>relationship_strain = w₁(miscommunication) + w₂(incompatible goals) + w₃(external stress) + w₄(personality issues) + w₅(past baggage) + c</strong></p>

<p>We love single-factor explanations because they’re simple. But life is <strong>multivariate</strong>, not binary.</p>

<hr />

<p>Ever since I started viewing life through this lens — not the <em>linear</em> part of Linear Regression, but the <em>weighted combination</em> part — I’ve become less judgmental, more curious, and surprisingly more forgiving.</p>

<p>Because nothing “just happens.”</p>

<figure>
  <img src="/assets/images/2025-10-16-life-is-linear-regression/image3.png" alt="Life as a sum of weighted factors" />
  <figcaption>Every outcome in life is the sum of multiple factors, each with its own weight — plus a bit of randomness</figcaption>
</figure>

<p><strong>Outcome = Σ (all factors × their weights) + some randomness</strong></p>

<p>And most of us are just bad at estimating the weights.</p>]]></content><author><name>Bejay</name></author><summary type="html"><![CDATA[Food for thought]]></summary></entry><entry><title type="html">Coding in the era of LLMs</title><link href="http://localhost:4000/2025/09/21/coding-in-the-era-of-llms.html" rel="alternate" type="text/html" title="Coding in the era of LLMs" /><published>2025-09-21T00:00:00+05:30</published><updated>2025-09-21T00:00:00+05:30</updated><id>http://localhost:4000/2025/09/21/coding-in-the-era-of-llms</id><content type="html" xml:base="http://localhost:4000/2025/09/21/coding-in-the-era-of-llms.html"><![CDATA[<p>The common people began to have access to “AI” since the launch of chatgpt in 2022 end (30th Nov, 2022 to be precise).
Before that AI was on a sci-fi domain for people. AI was being used for very specific day to day tasks but it was not as significant or head turner as perhaps a Ultron of Avengers fame.</p>

<p>This opened a new avenue. People started recognising the true power of AI. This was also felt in the software engineering career. People started to write code using these LLMs.</p>

<p>I also joined the bandwagon. It definitely felt helpful for certain well defined tasks.
But AI assisted coding was far from perfect. But it steadily started improving.</p>

<p>The turning point for me was cursor - the AI assisted IDE. I really started growing fond of the tab feature. By then the models had started to improve drastically too. The claude sonnet 3.5 + cursor duo turned out to be really a great duo. The tab feature specifically works for me because it is I who take care of the logic while the tab feature autocompletes it. It makes coding faster.</p>

<p>Then came cursor agent. It could really start building features on its own.</p>

<h2 id="the-dark-side-nobody-talks-about">The Dark Side Nobody Talks About</h2>

<p>But here’s the thing that worries me as I see more people jumping into this AI-powered coding revolution: <strong>not everyone should be coding with AI the same way.</strong></p>

<p>There’s a dangerous trend emerging, especially among beginners. People who are brand new to programming are treating AI as a magic wand that removes the need to actually learn to code. They’re “vibe coding” - throwing prompts at ChatGPT or Cursor and shipping whatever comes out without really understanding what’s happening under the hood.</p>

<h3 id="the-data-tells-a-sobering-story">The Data Tells a Sobering Story</h3>

<p>The numbers paint an uncomfortable picture. A 2025 study by METR tracked experienced developers and found something shocking: when using AI tools, developers actually took 19% longer to complete tasks compared to working without AI. Even more troubling? The developers thought they were 20% faster. That’s a 39-point gap between perception and reality.</p>

<p>Trust in AI coding tools has been plummeting too. Stack Overflow’s 2025 survey shows developer trust in AI output accuracy dropped from 43% in 2024 to just 33% in 2025. The favorability of adding AI tools to workflows fell from 72% to 60% in the same period.</p>

<h3 id="the-beginners-trap">The Beginner’s Trap</h3>

<p>For beginners, the risks are even more severe. When you don’t have the fundamentals, you can’t tell when AI is wrong. And it’s wrong more often than you’d think.</p>

<p>Consider these real problems people are facing:</p>

<ul>
  <li>Up to 30% of packages suggested by AI tools don’t even exist - they’re hallucinated, creating security vulnerabilities</li>
  <li>40% of AI-generated database queries are vulnerable to SQL injection attacks</li>
  <li>AI often puts security checks on the client side instead of the server</li>
  <li>Hardcoded API keys and secrets frequently appear in generated code</li>
</ul>

<p>One experienced developer shared how an AI-generated script locked them out of root access because they asked it to “make it super secure” without reviewing the implementation. These aren’t edge cases. They’re common patterns.</p>

<h3 id="you-still-need-to-learn-the-fundamentals">You Still Need to Learn the Fundamentals</h3>

<p>Here’s what the experts are saying: AI coding without a foundation is like letting someone who’s never flown a plane sit in the cockpit and take off in automated mode. It sounds crazy when you put it that way, right?</p>

<p>As one senior product manager at Qt Group put it: “If junior developers generate code with AI assistants and deploy the code to digital products without truly understanding it, then they run into the risk of introducing suboptimal code. Whenever junior developers use AI-generated code, they’re not really learning how to write and review code themselves.”</p>

<p>The most successful approach isn’t to avoid AI - that would be career suicide at this point. According to a 2024 developer survey, 76% of developers are using or planning to use AI coding assistants. The market for AI coding tools was valued at $5.5 billion in 2024 and is projected to hit $47.3 billion by 2034.</p>

<p>But you need to <strong>earn the right</strong> to use these tools effectively.</p>

<h3 id="the-right-way-to-learn-with-ai">The Right Way to Learn with AI</h3>

<p>Think of AI as a senior developer looking over your shoulder, not as a replacement for your brain. Here’s what actually works:</p>

<ol>
  <li>
    <p><strong>Learn the fundamentals first</strong>. Understand variables, control flow, data structures, functions. Write enough code manually that you can spot when something looks wrong.</p>
  </li>
  <li>
    <p><strong>Write it yourself, then compare</strong>. Try solving a problem on your own first. Then ask AI how it would do it. See what’s different. Learn from the gaps.</p>
  </li>
  <li>
    <p><strong>Never deploy code you don’t understand</strong>. If AI generates something and you can’t explain what each part does, you’re not ready to use it. Debug it. Break it. Fix it. Make it yours.</p>
  </li>
  <li>
    <p><strong>Use AI to augment, not replace, learning</strong>. AI is incredible for explaining concepts, suggesting improvements, and catching bugs. But it can’t build your mental model of how code works.</p>
  </li>
</ol>

<p>The developers who will thrive aren’t the ones who can write the best prompts. They’re the ones who know enough to recognize when AI is leading them astray.</p>

<h3 id="looking-forward">Looking Forward</h3>

<p>AI coding tools are only going to get better. Models are improving, context windows are expanding (Cursor now offers 1M+ token context windows), and response times are getting faster. We’re seeing the emergence of autonomous coding agents that can handle entire features.</p>

<p>But the gap between those who understand what’s happening and those who are just prompt-engineering their way through is going to become a chasm. Companies are already learning what happens when their codebases get infiltrated with AI-generated code at scale. We’re seeing bigger incidents with slower resolution times because the people trying to fix problems don’t understand the code that created them.</p>

<h2 id="my-take">My Take</h2>

<p>I love AI coding tools. The cursor + claude combo has genuinely made me more productive. But I was coding since before these tools existed. I had spent years debugging obscure errors, refactoring messy code, and building that intuition for what good code looks like. That foundation is why I can tell when cursor’s suggestions are brilliant and when they’re subtly broken.</p>

<p>I see people jumping straight to AI without that foundation, and honestly, it makes me a bit nervous for them. Not in a gatekeeping way - I’m genuinely excited about more people getting into coding. But there’s a difference between using AI as a productivity booster and using it as a crutch to avoid learning.</p>

<p>When I use the tab feature, I already know what I want to build. The AI just saves me the typing. When I use cursor agent, I review every change it makes because I know what to look for. That’s the difference.</p>

<p>Look, I’m not going to tell you how to learn. Maybe you’ll figure out your own path that works better. But from where I’m standing, having gone through both worlds - the pre-AI grind and the AI-assisted present - the people who seem to struggle the most are the ones who skipped straight to step two.</p>

<p>The cursor + claude duo is incredible. But it’s incredible <em>because</em> I know what I’m doing. Without that, it’s just a fancy autocomplete that occasionally leads you off a cliff.</p>

<p>That’s my experience anyway. Your mileage may vary.</p>]]></content><author><name>Bejay</name></author><summary type="html"><![CDATA[My thought dump on AI and coding]]></summary></entry><entry><title type="html">Your Feelings Lie to You (Sometimes)</title><link href="http://localhost:4000/2025/04/15/your-feelings-lie-to-you-sometimes.html" rel="alternate" type="text/html" title="Your Feelings Lie to You (Sometimes)" /><published>2025-04-15T00:00:00+05:30</published><updated>2025-04-15T00:00:00+05:30</updated><id>http://localhost:4000/2025/04/15/your-feelings-lie-to-you-sometimes</id><content type="html" xml:base="http://localhost:4000/2025/04/15/your-feelings-lie-to-you-sometimes.html"><![CDATA[<h2 id="we-are-human-beings-we-feel-things-and-thats-beautiful">We are human beings. We feel things. And that’s beautiful.</h2>

<p>But we also have a gift from evolution called the prefrontal cortex, the logical center of the brain.
Pop culture constantly pushes the idea of “follow your heart” or “do what feels right.”
But honestly? That’s not always the best way to live.</p>

<p>I would always suggest: use data, write stuff down, and then make decisions.
After that, if you fail so be it.
At least you made the best possible decision with the information you had at the time.</p>

<h2 id="emotions-are-amazing-but">Emotions Are Amazing, But…</h2>

<p>Don’t get me wrong.
I’m not some uptight dude who only thinks logically 24/7.
Emotions are wonderful. They make us human. They give life meaning.</p>

<p>What we feel is real to us.
But at any given point, making informed decisions will serve us better.</p>

<p>Now, I’m not saying you need to freeze and over-analyze every tiny choice.
Instead, ask yourself a simple question:</p>

<blockquote>
  <p><strong>Is the decision easily reversible?</strong></p>

  <p>If yes, go ahead, act fast.</p>

  <p>If no, take a step back, breathe, and analyze properly.</p>
</blockquote>

<h2 id="when-you-dont-feel-like-it">When You Don’t Feel Like It</h2>

<p>Let’s look at another side of this.
Sometimes you don’t feel like doing something.
You might not feel motivated. You might not feel happy.
You might even want to do something… but just not feel like it.</p>

<p>In those moments, you have to let your logical brain drive you even more.</p>

<p>Nowadays, almost everyone battles some form of anxiety or depression.
In those states, your feelings and your thoughts will lie to you.
You’ll think you can’t. You’ll feel like it’s pointless.</p>

<p>That’s exactly when practicing data-driven, logical action becomes your superpower.
I am not saying one should suppress emotions, it’s about not letting temporary feelings sabotage your trajectory in the long run.</p>

<h2 id="who-am-i">Who Am I?</h2>

<p>I’m a 28-year-old software engineer.
Here’s a quick snapshot of my journey:</p>

<ul>
  <li>Graduated B.Tech in ECE — 2019</li>
  <li>Completed M.Tech during the COVID years — 2021</li>
  <li>Started a PhD, dropped out after a year</li>
  <li>Prepared for interviews, hustled hard</li>
  <li>Finally joined a startup in October 2022</li>
</ul>

<p>It’s been a messy, beautiful ride.
And through it all, what I have learnt is to not trust my feelings blindly, but trust the process.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Feelings are important.
But for the most important decisions in life, let the logic lead.</p>

<p>Write things down.
Check the data.
Think for a minute longer.
And then act with all the conviction.</p>

<p>Feel your feelings, but don’t be ruled by them.</p>]]></content><author><name>Bejay</name></author><summary type="html"><![CDATA[My exploration of how emotions and logic shape our decision-making process]]></summary></entry></feed>