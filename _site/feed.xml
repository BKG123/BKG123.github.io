<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-28T00:04:54+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Bejay‚Äôs Website</title><subtitle>Welcome to my little corner of the internet. Full-time engineer passionate about GenAI, building magical experiences with clarity and logic.</subtitle><entry><title type="html">Reel Craft</title><link href="http://localhost:4000/2025/10/28/reel-craft.html" rel="alternate" type="text/html" title="Reel Craft" /><published>2025-10-28T00:00:00+05:30</published><updated>2025-10-28T00:00:00+05:30</updated><id>http://localhost:4000/2025/10/28/reel-craft</id><content type="html" xml:base="http://localhost:4000/2025/10/28/reel-craft.html"><![CDATA[<hr />

<h2 id="layout-post-title-reelcraft-the-ai-pipeline-that-turns-long-articles-into-viral-shorts-date-2025-10-28-description-how-we-built-reelcraftan-automated-system-using-gemini-ai-pexels-and-ffmpegto-convert-any-web-article-into-an-engaging-30-60-second-short-form-video-in-one-click-author-bejay-tags-ai-videogen-llm-gemini-python-fastapi-acknowledgment-built-with-the-visionary-zeal-of-paras-and-the-technical-brilliance-of-aakash">layout: post title: ‚ÄúReelCraft: The AI Pipeline That Turns Long Articles into Viral Shorts‚Äù date: 2025-10-28 description: ‚ÄúHow we built ReelCraft‚Äîan automated system using Gemini AI, Pexels, and FFmpeg‚Äîto convert any web article into an engaging 30-60 second short-form video in one click.‚Äù author: ‚ÄúBejay‚Äù tags: [AI, VideoGen, LLM, Gemini, Python, FastAPI] acknowledgment: ‚ÄúBuilt with the visionary zeal of Paras and the technical brilliance of Aakash.‚Äù</h2>

<h2 id="introduction-the-short-form-addiction-dilemma">Introduction: The Short-Form Addiction Dilemma</h2>

<p>It was during the last of <strong>Ninety Days</strong> (early 2024), a hyper-productive sprint where our team was shipping app prototypes weekly. For one such prototype, <strong>Paras</strong> (our CEO) had a funky, yet brilliant, hypothesis: almost everyone in the tech sphere <strong>wants to be productive</strong> and read long-form content, but is simultaneously <strong>addicted to short-form content</strong> (Reels, TikToks, Shorts). The addiction was real, and it was a problem begging for an AI solution.</p>

<p>The goal was simple: bridge the gap. Turn the boring, text-heavy productivity content into the snackable, engaging format of a reel.</p>

<p>I was entrusted with building the initial Proof of Concept (PoC) along with my colleague and best friend, <strong>Aakash</strong>. We had a blast. Fast forward to now (October 2025), I‚Äôve decided to reproduce the entire pipeline, refine it, and document the process. We called it <strong>ReelCraft</strong>.</p>

<p><strong>ReelCraft</strong> automatically transforms any web article into a polished, 30‚Äì60 second vertical video, complete with narration, stock media, and background music.</p>

<hr />

<h2 id="what-is-reelcraft-the-end-to-end-automation-engine-">What is ReelCraft? The End-to-End Automation Engine üé¨</h2>

<p>ReelCraft is a fully automated video generation pipeline. You paste an article URL, and out comes a professional-grade short video, perfectly formatted for social media.</p>

<h3 id="core-features-at-a-glance">Core Features at a Glance</h3>

<ul>
  <li><strong>Automatic Script Generation:</strong> Gemini AI converts dense articles into <strong>7-15 punchy, scene-based scripts</strong> optimized for a fast-paced reel format.</li>
  <li><strong>AI-Powered Voice Over:</strong> Natural-sounding voice narration is generated for each scene using <strong>Gemini Text-to-Speech (TTS)</strong>.</li>
  <li><strong>Smart Asset Selection:</strong> The pipeline automatically finds and downloads relevant <strong>images and videos</strong> from <strong>Pexels</strong> based on keywords generated alongside the script.</li>
  <li><strong>Professional Composition:</strong> <strong>FFmpeg</strong> stitches the visual assets, audio, and background music together into a vertical video (720x1280).</li>
  <li><strong>Modern Web UI:</strong> A user-friendly, responsive interface with <strong>WebSocket</strong> integration for <strong>real-time progress tracking</strong>.</li>
</ul>

<h3 id="the-flow-url-to-reel">The Flow: URL to Reel</h3>

<p>The entire process is a streamlined, five-step pipeline:</p>

\[\text{Article URL} \xrightarrow{\text{Content Extraction}} \text{Script Generation} \xrightarrow{\text{Audio Generation}} \text{Asset Download} \xrightarrow{\text{Video Editing}} \text{Final Video}\]

<hr />

<h2 id="system-design-how-it-all-fits-together">System Design: How It All Fits Together</h2>

<p>The architecture is built for speed and concurrency, leveraging asynchronous Python (<code class="language-plaintext highlighter-rouge">asyncio</code>) and specialized APIs for each task.</p>

<h3 id="1-content-extraction-firecrawl">1. Content Extraction (FireCrawl)</h3>

<p>The first challenge is getting clean, structured text from a messy web page.</p>

<ul>
  <li>We use <strong>FireCrawl</strong> for this. It handles JavaScript-rendered content and returns the article in <strong>clean Markdown</strong>, ready for the LLM.</li>
</ul>

<h3 id="2-script-generation-gemini-ai">2. Script Generation (Gemini AI)</h3>

<p>This is the <strong>core intelligence</strong>. A prompt is sent to a <strong>Gemini</strong> model (likely Gemini 2.5 Flash for speed) that instructs it to perform a few crucial tasks:</p>

<ul>
  <li>Create a compelling <strong>title</strong>.</li>
  <li>Break the article down into <strong>7-15 scenes</strong> (max 60 seconds total duration).</li>
  <li>For <em>each scene</em>, generate the <strong>narration text</strong> and <strong>3 descriptive keywords</strong> for visual search. This separation of concerns is key for quality output. The output is structured JSON.</li>
</ul>

<h3 id="3-audio--asset-generation-parallel-processing">3. Audio &amp; Asset Generation (Parallel Processing)</h3>

<p>These two heavy tasks run <strong>concurrently</strong> to minimize latency:</p>

<ul>
  <li><strong>Audio Generation:</strong> The narration for all 7-15 scenes is converted to separate <code class="language-plaintext highlighter-rouge">.wav</code> files using the <strong>Gemini TTS API</strong>. We use an <code class="language-plaintext highlighter-rouge">asyncio.Semaphore</code> to limit concurrent requests (e.g., max 3 at once) to avoid API rate limits.</li>
  <li><strong>Asset Download:</strong> Using the keywords from the script, the pipeline calls the <strong>Pexels API</strong> to download the most relevant image or video for each scene, ensuring it matches the required vertical aspect ratio.</li>
</ul>

<h3 id="4-video-composition-ffmpeg">4. Video Composition (FFmpeg)</h3>

<p>The final assembly line, driven by the robust <strong>FFmpeg</strong> utility:</p>

<ul>
  <li><strong>Stitching:</strong> Visual assets are concatenated sequentially.</li>
  <li><strong>Duration Sync:</strong> This was a key challenge: the audio duration must precisely match the visual duration. FFmpeg is used to <strong>automatically adjust the audio tempo</strong> (speed) to align perfectly with the scene‚Äôs visual clip length, ensuring a snappy flow.</li>
  <li><strong>Audio Mixing:</strong> The voice-over audio track is layered with a background music track, with the voice-over volume boosted and the music volume suppressed (e.g., <code class="language-plaintext highlighter-rouge">background_volume = 0.2</code>).</li>
  <li><strong>Effects:</strong> Basic effects (like zoom/pan for static images) are applied to maintain visual dynamism.</li>
</ul>

<h3 id="the-fastapi-backend--web-ui">The FastAPI Backend &amp; Web UI</h3>

<p>The entire system is wrapped in a <strong>FastAPI</strong> application, providing both a <strong>REST API</strong> (<code class="language-plaintext highlighter-rouge">/api/generate-video</code>) and a <strong>WebSocket</strong> connection (<code class="language-plaintext highlighter-rouge">/ws</code>) to stream real-time progress updates to the frontend. This allows users to see <em>exactly</em> when the script is generating, audio is downloading, and FFmpeg is composing.</p>

<hr />

<h2 id="technical-learnings--design-decisions">Technical Learnings &amp; Design Decisions</h2>

<h3 id="decision-1-structured-json-output-for-llm">Decision 1: Structured JSON Output for LLM</h3>

<p>Initially, we just asked the LLM for a script. It was messy.</p>

<ul>
  <li><strong>Solution:</strong> We enforced a strict JSON schema that requires the scene number, script text, and a dedicated <code class="language-plaintext highlighter-rouge">asset_keywords</code> array for each scene. This structured output is vital‚Äîit makes the downstream parallel processing of audio and visuals deterministic and reliable.</li>
</ul>

<h3 id="decision-2-prioritizing-parallelism-for-ux">Decision 2: Prioritizing Parallelism for UX</h3>

<p>The full pipeline (extraction to final video) is inherently slow due to multiple API calls (Gemini, Pexels).</p>

<ul>
  <li><strong>Insight:</strong> Waiting 3 minutes for a video is better than waiting 10 minutes.</li>
  <li><strong>Implementation:</strong> By running the <strong>Audio Generation</strong> and <strong>Asset Download</strong> steps concurrently (<code class="language-plaintext highlighter-rouge">asyncio.gather</code>), we slashed the total processing time significantly, resulting in a much better user experience.</li>
</ul>

<h3 id="decision-3-tempo-adjustment-for-seamless-sync">Decision 3: Tempo Adjustment for Seamless Sync</h3>

<p>A silent video is useless for a reel.</p>

<ul>
  <li><strong>Problem:</strong> The Gemini TTS audio for a scene might be 5.2 seconds long, but the chosen Pexels clip might be 6 seconds. A simple cut creates a jarring silence.</li>
  <li><strong>Solution:</strong> We calculate the required speed adjustment factor ($\frac{\text{Visual Duration}}{\text{Audio Duration}}$) and apply it using FFmpeg‚Äôs <code class="language-plaintext highlighter-rouge">atempo</code> filter. This subtly speeds up or slows down the voice-over so it ends precisely when the visual scene transitions.</li>
</ul>

<h3 id="decision-4-monitoring-with-langfuse">Decision 4: Monitoring with Langfuse</h3>

<p>To move beyond ‚Äúit works on my machine,‚Äù we integrated <strong>Langfuse</strong>.</p>

<ul>
  <li><strong>Benefit:</strong> It tracks every single LLM call (scripting, TTS), their latency, and token usage. This allows us to debug prompt failures and, critically, monitor API costs.</li>
</ul>

<hr />

<h2 id="reelcraft-technical-stack">ReelCraft Technical Stack</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Tool / API</th>
      <th style="text-align: left">Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>LLM &amp; TTS</strong></td>
      <td style="text-align: left">Google Gemini API (<code class="language-plaintext highlighter-rouge">google-genai</code>)</td>
      <td style="text-align: left">Scripting and Voice-Over Generation</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Content Extraction</strong></td>
      <td style="text-align: left">FireCrawl API (<code class="language-plaintext highlighter-rouge">firecrawl-py</code>)</td>
      <td style="text-align: left">Cleanly scrape article text from any URL</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Stock Media</strong></td>
      <td style="text-align: left">Pexels API (<code class="language-plaintext highlighter-rouge">requests</code>)</td>
      <td style="text-align: left">Search and download royalty-free video/images</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Video Editing</strong></td>
      <td style="text-align: left">FFmpeg (<code class="language-plaintext highlighter-rouge">ffmpeg-python</code>)</td>
      <td style="text-align: left">Final composition, stitching, and audio mixing</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Web Server &amp; API</strong></td>
      <td style="text-align: left">FastAPI, Uvicorn, WebSocket</td>
      <td style="text-align: left">Real-time progress tracking and API endpoints</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Monitoring</strong></td>
      <td style="text-align: left">Langfuse</td>
      <td style="text-align: left">Tracing, debugging, and cost management</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="whats-next-the-roadmap-">What‚Äôs Next: The Roadmap üöÄ</h2>

<p>While ReelCraft is a functional and polished PoC, there‚Äôs always more to build:</p>

<ul>
  <li><strong>Subtitle Generation:</strong> Crucial for short-form content consumption (watching without sound).</li>
  <li><strong>Customization:</strong> Adding options for custom background music, different TTS voices, and font/style controls.</li>
  <li><strong>Batch Processing:</strong> Allowing users to queue up multiple articles for video generation.</li>
  <li><strong>Evals:</strong> Developing a proper LLM-as-a-Judge evaluation framework to quantitatively assess the quality of the generated script and asset keywords.</li>
</ul>

<p>The blend of an LLM‚Äôs creativity with robust media processing tools like FFmpeg proved to be an incredibly powerful combination. We took a common problem‚Äîthe attention-span gap‚Äîand solved it with a fully automated, scalable pipeline.</p>

<p><strong>Check out the repository and try the API!</strong></p>

<hr />

<p><strong>GitHub</strong>: [repository-url-here]</p>

<p><strong>API Docs</strong>: <a href="https://www.google.com/search?q=http://localhost:8000/docs">http://localhost:8000/docs</a> (when running locally)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Frame AI: Building an AI-Powered Photography Assistant</title><link href="http://localhost:4000/2025/10/20/frame-ai.html" rel="alternate" type="text/html" title="Frame AI: Building an AI-Powered Photography Assistant" /><published>2025-10-20T00:00:00+05:30</published><updated>2025-10-20T00:00:00+05:30</updated><id>http://localhost:4000/2025/10/20/frame-ai</id><content type="html" xml:base="http://localhost:4000/2025/10/20/frame-ai.html"><![CDATA[<h2 id="introduction-the-mobile-photography-itch">Introduction: The Mobile Photography Itch</h2>

<p>Every developer will tell you this: ‚ÄúI want to work on a side project to improve my portfolio.‚Äù But almost all of them will also admit they never got around to building it. I wanted to break that loop, so I started hunting for ideas. I‚Äôve always been fascinated by photography. Never professional-level good, but I can click half-decent pics on my iPhone. One of my friends introduced me to amateur photography principles ‚Äî rule of thirds, leading lines, proper lighting. I try to keep those in mind while taking snaps. More often than not, I fail, lol. That‚Äôs when the idea hit me: What if I could analyze images using vision LLMs (like Gemini 2.5 Flash) by prompting them correctly to check alignment with widely accepted photography rules? As I was iterating on the project, Google launched Gemini 2.5 Flash Image (nicknamed ‚Äúnano-banana‚Äù by the developer community) in August 2025 ‚Äî a breakthrough in image generation and editing that hit #1 on LMArena‚Äôs leaderboards. I thought, why not edit the images based on the analysis? So yeah, in short: Frame AI analyzes images and critiques them, and you can enhance images using Gemini 2.5 Flash Image.</p>

<p><strong>Try it here</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a><br />
<strong>GitHub</strong>: <a href="https://github.com/BKG123/frame-ai" target="_blank" rel="noopener noreferrer">https://github.com/BKG123/frame-ai</a><br />
<strong>Video Demo</strong>: <a href="/projects/frame-ai/">Watch it in action ‚Üí</a></p>

<p><strong>Why build Frame AI as a side project:</strong></p>
<ul>
  <li>Bridge the gap between taking photos and knowing how to improve them</li>
  <li>Explore the AI + photography intersection</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/app-1.webp" alt="Sample app image" />
  <figcaption>Frame AI Home</figcaption>
</figure>

<hr />

<h2 id="what-frame-ai-actually-does">What Frame AI Actually Does</h2>

<p>So the core idea is simple: an AI assistant that actually understands photography principles.</p>

<p>Not just ‚Äúmake it prettier‚Äù - I wanted actual compositional feedback. There are fixed rules in photography: rule of thirds, leading lines, lighting, balance, and so on. Frame AI analyzes your photo against these principles and tells you what could be better.</p>

<p><strong>Two main things it does:</strong></p>

<ol>
  <li><strong>Analysis</strong>: What‚Äôs working, what‚Äôs not, and why</li>
  <li><strong>Enhancement</strong>: AI-powered edits based on the analysis</li>
</ol>

<p><strong>The interesting twist</strong>: Instructions generated separately -&gt; So basically I make a separate call to the LLM to generate 3 distinct editing prompts from the analysis, each focusing on different aspects: technical perfection, atmospheric reinterpretation, and conceptual narrative. It also has access to the best practices of Gemini 2.5 Flash Image prompting techniques.</p>

<p>Earlier I was passing the analysis directly to the image generation prompt but the output wasn‚Äôt that good. My intuition was that Gemini 2.5 Flash Image excels at generating or editing images when given clear, specific instructions. I shouldn‚Äôt depend on it to reason and then generate ‚Äî separation of concerns works better.</p>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/quick-analysis.webp" alt="Sample analysis output" />
  <figcaption>Digestible, bullet-pointed feedback that actually helps</figcaption>
</figure>

<hr />

<h2 id="system-design-how-it-all-fits-together">System Design: How It All Fits Together</h2>

<p><strong>High-level architecture:</strong></p>

<p>User uploads image ‚Üí FastAPI backend ‚Üí Image processing &amp; caching layer ‚Üí LLM analysis ‚Üí Stored in DB ‚Üí Enhance Image Trigger ‚Üí 3 editing prompts generated from analysis ‚Üí 3 images generated in parallel using Gemini 2.5 Flash Image</p>

<p><strong>Key components:</strong></p>

<ul>
  <li><strong>Database</strong>: SQLite</li>
  <li><strong>LLM Integration</strong>: Gemini 2.5 Flash for analysis, Gemini 2.5 Flash Lite for JSON structuring, Gemini 2.5 Flash Image for enhancement</li>
  <li><strong>Caching Strategy</strong>: Content-based hashing prevents duplicate processing (performance + cost optimization)</li>
  <li><strong>API Design</strong>: RESTful endpoints with Server-Sent Events for streaming analysis</li>
</ul>

<figure>
  <img src="/assets/images/2025-10-20-frame-ai/system-design.webp" alt="System architecture diagram" data-lightbox="image" />
  <figcaption>Clean architectural overview of Frame AI</figcaption>
</figure>

<p>As mentioned earlier, the product has two main parts: image analysis and image enhancement.</p>

<p><strong>Image Analysis Flow:</strong></p>
<ol>
  <li>Frontend calls <code class="language-plaintext highlighter-rouge">/upload</code> API with image file</li>
  <li>Backend workflow:
    <ul>
      <li>Content-based hash is generated from file bytes (fixes duplicate detection edge cases)</li>
      <li>Image stored permanently in <code class="language-plaintext highlighter-rouge">static/uploaded_images/{hash}.{ext}</code> (deduplication happens here)</li>
      <li>Check SQLite cache ‚Äî if hash exists, stream cached analysis immediately</li>
      <li>For new images:
        <ul>
          <li>Temporary file created for processing</li>
          <li>Gemini 2.5 Flash call with image + photography best practices prompt</li>
          <li>LLM streams detailed analysis with scores (exposure, composition, lighting, overall)</li>
          <li>Analysis stored in SQLite against file hash</li>
          <li>Frontend receives analysis via Server-Sent Events (real-time streaming)</li>
        </ul>
      </li>
      <li>EXIF data extracted and stored for context</li>
    </ul>
  </li>
</ol>

<p><strong>Image Enhancement Flow:</strong></p>
<ol>
  <li>Frontend calls <code class="language-plaintext highlighter-rouge">/image/edit</code> API with file hash</li>
  <li>Backend workflow:
    <ul>
      <li>Fetch cached analysis from SQLite using hash</li>
      <li>Generate 3 distinct editing prompts via Gemini 2.5 Flash (with Gemini 2.5 Flash Image best practices as context)
        <ul>
          <li>Prompt 1: Technical perfection &amp; enhancement</li>
          <li>Prompt 2: Atmospheric &amp; mood reinterpretation</li>
          <li>Prompt 3: Conceptual &amp; narrative composite</li>
        </ul>
      </li>
      <li>Launch 3 parallel Gemini 2.5 Flash Image API calls (<code class="language-plaintext highlighter-rouge">asyncio.gather</code>)</li>
      <li>Each call returns: enhanced image + text description of changes</li>
      <li>Text descriptions converted to structured JSON via Gemini 2.5 Flash Lite (temperature = 0 for consistency)</li>
      <li>Return 3 enhanced images with metadata to frontend</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="design-decisions--lessons-learned">Design Decisions &amp; Lessons Learned</h2>

<h3 id="decision-1-content-based-hashing-for-caching">Decision 1: Content-Based Hashing for Caching</h3>

<p><strong>Why I needed this:</strong></p>
<ul>
  <li>The analysis needs to be reused for enhancement (same image = same analysis)</li>
  <li>Duplicate uploads waste API calls and cost me money</li>
  <li>Better UX: instant results if someone already uploaded that image</li>
</ul>

<p><strong>How I figured this out:</strong></p>
<ul>
  <li><strong>First attempt</strong>: Filename + IP-based caching
    <ul>
      <li>Problem: Same filename, different image = cache collision (oops)</li>
      <li>Problem: Storing IP addresses = PII concerns (not great)</li>
    </ul>
  </li>
  <li><strong>Final solution</strong>: Content-based hashing (SHA-256 of file bytes)
    <ul>
      <li>Same image content ‚Üí same hash ‚Üí reliable cache hit</li>
      <li>Different images ‚Üí different hashes ‚Üí no false positives</li>
      <li>Privacy-friendly: no PII stored</li>
    </ul>
  </li>
</ul>

<h3 id="decision-2-three-parallel-image-variations">Decision 2: Three Parallel Image Variations</h3>

<p><strong>Why not just one enhanced image?</strong></p>
<ul>
  <li>Photography enhancement is super subjective</li>
  <li>The analysis covers multiple things (technical stuff, artistic vibes, mood)</li>
  <li>People want choice ‚Äî what looks ‚Äúbetter‚Äù varies by taste and what you‚Äôre using it for</li>
  <li>Three variations let you see different creative directions</li>
</ul>

<p><strong>The three approaches I settled on:</strong></p>
<ol>
  <li><strong>Technical perfection</strong>: Fix exposure, sharpen details, recover dynamic range</li>
  <li><strong>Atmospheric reinterpretation</strong>: Transform mood through color grading and lighting</li>
  <li><strong>Conceptual narrative</strong>: Reimagine the story (subtle compositing, creative edits)</li>
</ol>

<p><strong>Hyperparameter tuning:</strong></p>
<ul>
  <li><strong>Analysis LLM</strong> (Gemini 2.5 Flash): temperature = 0.3-0.5 (balance creativity with structure)</li>
  <li><strong>JSON structuring</strong> (Gemini 2.5 Flash Lite): temperature = 0 (deterministic output)</li>
  <li><strong>Prompt generation</strong> (Gemini 2.5 Flash): temperature = 0.5 (creative but focused)</li>
</ul>

<h3 id="decision-3-separate-llm-call-for-prompt-generation">Decision 3: Separate LLM Call for Prompt Generation</h3>

<p><strong>What I tried first</strong>: Passing the analysis directly to Gemini 2.5 Flash Image</p>
<ul>
  <li>Problem: Output quality was all over the place</li>
  <li>The image model struggled to pull out actionable edits from my verbose analysis</li>
</ul>

<p><strong>What actually worked</strong>: Dedicated prompt generation step</p>
<ul>
  <li>Generate 3 specific editing prompts via Gemini 2.5 Flash</li>
  <li>Include <a href="https://ai.google.dev/gemini-api/docs/image-generation#best-practices" target="_blank" rel="noopener noreferrer">Gemini 2.5 Flash Image best practices</a> as context</li>
  <li>Pass clean, focused instructions to the image model</li>
</ul>

<p><strong>Why this works better</strong>: Gemini 2.5 Flash Image is great at following clear, step-by-step instructions ‚Äî but I shouldn‚Äôt ask it to reason about photography theory <em>and</em> generate images at the same time. Separation of concerns wins again.</p>

<p><strong>Results</strong>: Way more precise, actionable edits. The enhancements became subtle but actually comprehensible instead of over-processed.</p>

<h3 id="decision-4-design-constraints--negative-prompting">Decision 4: Design Constraints &amp; Negative Prompting</h3>

<p><strong>What I explicitly told the models NOT to do:</strong></p>
<ul>
  <li>Don‚Äôt add objects/people that weren‚Äôt in the original (authenticity matters for photography)</li>
  <li>Don‚Äôt rotate or change orientation (preserve the photographer‚Äôs intent)</li>
  <li>Don‚Äôt over-process to the point of looking fake</li>
</ul>

<p><strong>Some UX stuff I care about:</strong></p>
<ul>
  <li>Keep feedback digestible ‚Äî no one reads walls of text</li>
  <li>Focus on enhancement, not transformation</li>
  <li>Make AI feedback feel specific, not generic</li>
</ul>

<h3 id="trade-offs-cost-vs-quality">Trade-offs: Cost vs. Quality</h3>

<p><strong>My current approach</strong>: Optimize for quality first</p>
<ul>
  <li>I‚Äôm making multiple LLM calls per request (analysis + prompt generation + JSON structuring)</li>
  <li>Not worrying about cost during initial development</li>
  <li>Philosophy: Get the best results first, then optimize later</li>
</ul>

<p><strong>Future optimizations I‚Äôm thinking about:</strong></p>
<ul>
  <li>Replace JSON-generating LLM calls with markdown parsers</li>
  <li>Use Python imaging libraries (Pillow) for simple adjustments instead of hitting the image generation API</li>
  <li>Maybe fine-tune an open-source model for analysis</li>
  <li>Batch processing for multiple images</li>
</ul>

<h3 id="the-metric-saga">The Metric Saga</h3>

<p><strong>What happened</strong>: I added metrics tracking. Then I removed it.</p>

<p><strong>Why?</strong></p>
<ul>
  <li>I tried to give some quantitative insights using LLMs to calculate metrics like sharpness, color composition</li>
  <li>Problem: The generated image is completely new, not a modified version of the original</li>
  <li>So metrics might show ‚Äúimprovement‚Äù but they‚Äôre comparing apples to oranges - not super useful</li>
</ul>

<div class="image-grid">
  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/og_image.webp" alt="Original photo" />
    <figcaption>Original</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var1.webp" alt="Enhanced variation 1" />
    <figcaption>Variation 1</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var2.webp" alt="Enhanced variation 2" />
    <figcaption>Variation 2</figcaption>
  </figure>

  <figure>
    <img src="/assets/images/2025-10-20-frame-ai/var3.webp" alt="Enhanced variation 3" />
    <figcaption>Variation 3</figcaption>
  </figure>
</div>

<hr />

<h2 id="some-challenges--solutions">Some Challenges &amp; Solutions</h2>

<h3 id="front-end-development">Front end development</h3>

<p><strong>Problem</strong>: I know very little FE dev</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>In this day and age you can‚Äôt really bracket yourself into FE and BE. And also I couldn‚Äôt really show my product via APIs!</li>
  <li>Claude Code came to the rescue. It really spun up the FE based on my instructions.
There were multiple iterations in a loop of claude doing something in FE -&gt; Me not liking it -&gt; putting ss in the input and asking it to rebuild. Also used Cursor‚Äôs in built browser tool to let it gather info of current FE design and make some tweaks.</li>
  <li>Attached the index.html file to Gemini and chatgpt and asked them to make it better but it was more or less the same thing.</li>
  <li>I felt that the design was not that great so used lovable to make some changes. Turns out, you can‚Äôt load an existing repo there. So created a dummy repo and pushed my code base there, connected to lovable. It couldn‚Äôt preview, but it added some animations to the buttons.</li>
  <li>The current state of the FE is not great but it gets the job done</li>
</ul>

<h3 id="orientation">Orientation</h3>

<p><strong>Problem</strong>:</p>
<ul>
  <li>The edited image came out in incorrect orientation because probably, iPhone photos (with which I was testing) had incorrect EXIF rotation metadata</li>
</ul>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Used Pillow‚Äôs <code class="language-plaintext highlighter-rouge">ImageOps.exif_transpose()</code> to auto-correct orientation before processing</li>
  <li>Explicitly added in the prompt to maintain orientation</li>
  <li>Both of these changes were suggested by Claude code and applied them at the same time -&gt; seemed to fix the problem</li>
</ul>

<h3 id="making-ai-feedback-actually-useful-not-generic">Making AI Feedback Actually Useful (Not Generic)</h3>

<p><strong>Problem</strong>: Early versions gave generic feedback like ‚Äúnice composition‚Äù or ‚Äúgood lighting‚Äù that didn‚Äôt help users improve.</p>

<p><strong>Solution</strong>:</p>
<ul>
  <li>Crafted a detailed system prompt with specific photography principles (rule of thirds, leading lines, etc.)</li>
  <li>Included EXIF data in the prompt so the LLM could explain technical settings</li>
  <li>Asked for structured feedback: strengths, improvements, professional tips</li>
  <li>Added numerical scores (1-10) for exposure, composition, lighting, overall</li>
  <li>I arrived at this after multiple iterations.</li>
</ul>

<p><strong>Example of specific vs. generic feedback</strong>:</p>
<ul>
  <li>‚ùå Generic: ‚ÄúThe lighting could be better‚Äù</li>
  <li>‚úÖ Specific: ‚ÄúThe main light source creates harsh shadows on the subject‚Äôs face. Try shooting during the ‚Äògolden hour‚Äô or using a diffuser to soften the light.‚Äù</li>
</ul>

<h3 id="evals-planned">Evals (planned)</h3>
<ul>
  <li>This actually is an extension of the previous point.</li>
  <li>Till now I haven‚Äôt written ‚ÄòLLM Evals‚Äô as such. Have some test cases which checks whether the json structure expected and some heuristics on length.</li>
  <li>Have plans to write proper evals for analysis - using LLM as a Judge method (have used this at work)</li>
  <li>Have some ideas about image evals from <a href="https://github.com/Storia-AI/image-eval" target="_blank" rel="noopener noreferrer">Storia-AI</a></li>
  <li>Will update this blog, after implementing evals</li>
</ul>

<h2 id="whats-next">What‚Äôs Next</h2>

<p><strong>Some improvements I want to make:</strong></p>
<ul>
  <li><strong>Evals</strong>: Any LLM-based app should have proper evals - can‚Äôt just depend on eyeballing things</li>
  <li><strong>Cost optimization</strong>: Consolidate LLM calls, maybe use open-source models for some tasks</li>
  <li><strong>Batch processing</strong>: Upload multiple photos, get bulk analysis</li>
  <li><strong>Feedback over time</strong>: Track a user‚Äôs uploads and help them get better at photography over time</li>
</ul>

<p><strong>Open questions I‚Äôm still thinking about:</strong></p>
<ul>
  <li>How do I balance automation with creative control? (Should I let users tweak prompts? Add sliders for ‚Äúenhancement intensity‚Äù?)</li>
  <li>What makes AI feedback feel ‚Äúauthentic‚Äù vs. generic? (Still experimenting with prompt engineering)</li>
  <li>Should I add a ‚Äúlearn from feedback‚Äù loop where users mark helpful vs. unhelpful critiques?</li>
</ul>

<hr />

<h2 id="conclusion-the-side-project-effect">Conclusion: The Side Project Effect</h2>

<p>Now that I‚Äôve shipped my first ‚ÄúSide Project‚Äù, I hope to be consistent and keep building new things at regular intervals. And also keep improving this one, obviously.</p>

<p><strong>Try it, break it, let me know what you think</strong>: <a href="https://frame-ai.bejayketanguin.com/" target="_blank" rel="noopener noreferrer">https://frame-ai.bejayketanguin.com/</a></p>

<hr />

<h2 id="more-from-my-blog">More from my blog</h2>

<ul>
  <li><strong><a href="/2025/04/15/your-feelings-lie-to-you-sometimes.html">Your Feelings Lie to You (Sometimes)</a></strong> - My exploration of how emotions and logic shape our decision-making process</li>
  <li><strong><a href="/2025/09/21/coding-in-the-era-of-llms.html">Coding in the era of LLMs</a></strong> - My thoughts on AI-assisted coding and the importance of learning fundamentals</li>
  <li><strong><a href="/2025/10/16/life-is-linear-regression.html">Everything in Life is Linear Regression</a></strong> - Why life‚Äôs complexities are best understood as weighted combinations of multiple factors</li>
</ul>]]></content><author><name>Bejay</name></author><category term="Tech" /><summary type="html"><![CDATA[How I built an AI system that analyzes and enhances photos while teaching me.]]></summary></entry><entry><title type="html">Everything in Life is Linear Regression</title><link href="http://localhost:4000/2025/10/16/life-is-linear-regression.html" rel="alternate" type="text/html" title="Everything in Life is Linear Regression" /><published>2025-10-16T00:00:00+05:30</published><updated>2025-10-16T00:00:00+05:30</updated><id>http://localhost:4000/2025/10/16/life-is-linear-regression</id><content type="html" xml:base="http://localhost:4000/2025/10/16/life-is-linear-regression.html"><![CDATA[<figure>
  <img src="/assets/images/2025-10-16-life-is-linear-regression/image1.webp" alt="Everything in Life is Linear Regression" />
  <figcaption>Life's complexities can be understood through the lens of weighted combinations</figcaption>
</figure>

<p>When I started learning ML, I was first introduced to Linear Regression. In short, it describes an algorithm where you can model a function using a linear expression:</p>

<p><strong>y = wx + c</strong></p>

<p>Largely similar to the equation of a straight line. Here, the value of <strong>y</strong> (dependent variable) changes with <strong>x</strong> (independent variable).</p>

<p>Now, if we extrapolate this to multiple independent variables:</p>

<p><strong>y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ‚Ä¶ + w‚Çôx‚Çô + c</strong></p>

<p>In most use cases of linear regression, this is the case. An outcome or output is dependent on multiple factors.</p>

<p>Suppose you‚Äôre modeling the house price of a city using linear regression. You‚Äôll find that historically, the price of a house depends on multiple factors ‚Äî area, number of rooms, sq footage, parking (available or not), and so on. The Linear Regression algorithm tries to find those coefficients ‚Äî w‚ÇÅ, w‚ÇÇ, ‚Ä¶ w‚Çô ‚Äî and we get a model (or equation) on which, if we feed in new values of x‚ÇÅ‚Ä¶x‚Çô, we can ‚Äúpredict‚Äù or ‚Äúestimate‚Äù the cost of the house in question.</p>

<p>Now, the idea of this blog is not to deep dive into LR. It‚Äôs because I seem to find a parallel between everything in life and this mathematical concept ‚Äî not the linear part, but the <strong>combination part</strong> where everything is a combination of multiple things with different scaling factors associated with each of them.</p>

<hr />

<h3 id="for-example">For example:</h3>

<p>Suppose you missed a train on a certain day. You become extremely angry and start blaming your mom for apparently ‚Äúmaking you late‚Äù by asking you to eat breakfast before leaving. But this is <strong>black-and-white thinking</strong> ‚Äî sure, it might have played a role. But there are other factors here as well to consider. Like the fact that you slept late last night despite knowing you have a train to catch the next day. Also, the traffic at that time was more than usual.</p>

<figure>
  <img src="/assets/images/2025-10-16-life-is-linear-regression/image2.webp" alt="Multiple factors contributing to missing a train" />
  <figcaption>Missing the train isn't about one factor ‚Äî it's a weighted combination of breakfast delay, waking up late, traffic, and more</figcaption>
</figure>

<p>If I were to put it in the equation:</p>

<p><strong>minutes_late = w‚ÇÅ(breakfast_delay) + w‚ÇÇ(woke_up_late) + w‚ÇÉ(traffic_level) + w‚ÇÑ(distance_to_station) + w‚ÇÖ(train_punctuality) + c</strong></p>

<p>Where:</p>
<ul>
  <li><strong>minutes_late</strong> = how many minutes late you arrived at the station (or how close you were to missing the train)</li>
  <li><strong>breakfast_delay</strong> = time spent on breakfast (in minutes)</li>
  <li><strong>woke_up_late</strong> = how late you woke up compared to planned time (in minutes)</li>
  <li><strong>traffic_level</strong> = traffic congestion factor (could be 1-10 scale, or actual delay in minutes)</li>
  <li><strong>distance_to_station</strong> = distance you need to travel (in km)</li>
  <li><strong>train_punctuality</strong> = how early/late the train typically runs (in minutes)</li>
  <li><strong>c</strong> = baseline constant (accounts for other unmeasured factors)</li>
</ul>

<p>The weights (w‚ÇÅ, w‚ÇÇ, w‚ÇÉ, etc.) represent how much each factor contributes to the outcome. For instance:</p>
<ul>
  <li>Maybe <strong>w‚ÇÇ</strong> is large because waking up late has a huge cascading effect</li>
  <li><strong>w‚ÇÅ</strong> might be small because breakfast only added 5 minutes</li>
  <li><strong>w‚ÇÉ</strong> could be moderate depending on how unpredictable traffic is</li>
</ul>

<hr />

<p>The more experiences I have in life, the more I resonate with this.</p>

<p>Now, I know real linear regression has assumptions about linearity and independence that life often violates. But as a mental model for thinking about multiple factors contributing to outcomes, it works surprisingly well.</p>

<p>This also helps me approach differences in opinions in a calmer and composed manner. Let‚Äôs say India wins a cricket match ‚Äî some say it was because of Virat‚Äôs ton. Some say it‚Äôs because of Bumrah‚Äôs fifer. Or some say it was because of Rohit‚Äôs quickfire 25 off 10 balls.</p>

<p>I say <strong>it‚Äôs all of that</strong>. Just with different weights.</p>

<hr />

<h3 id="another-example">Another example:</h3>

<p>Someone says, ‚ÄúThey broke up because he was toxic.‚Äù</p>

<p>But reality?</p>

<p><strong>relationship_strain = w‚ÇÅ(miscommunication) + w‚ÇÇ(incompatible goals) + w‚ÇÉ(external stress) + w‚ÇÑ(personality issues) + w‚ÇÖ(past baggage) + c</strong></p>

<p>We love single-factor explanations because they‚Äôre simple. But life is <strong>multivariate</strong>, not binary.</p>

<hr />

<p>Ever since I started viewing life through this lens ‚Äî not the <em>linear</em> part of Linear Regression, but the <em>weighted combination</em> part ‚Äî I‚Äôve become less judgmental, more curious, and surprisingly more forgiving.</p>

<p>Because nothing ‚Äújust happens.‚Äù</p>

<figure>
  <img src="/assets/images/2025-10-16-life-is-linear-regression/image3.webp" alt="Life as a sum of weighted factors" />
  <figcaption>Every outcome in life is the sum of multiple factors, each with its own weight ‚Äî plus a bit of randomness</figcaption>
</figure>

<p><strong>Outcome = Œ£ (all factors √ó their weights) + some randomness</strong></p>

<p>And most of us are just bad at estimating the weights.</p>

<hr />

<h2 id="more-from-my-blog">More from my blog</h2>

<ul>
  <li><strong><a href="/2025/10/20/frame-ai.html">Frame AI: Building an AI-Powered Photography Assistant</a></strong> - How I built an AI system that analyzes and enhances photos while teaching me</li>
  <li><strong><a href="/2025/04/15/your-feelings-lie-to-you-sometimes.html">Your Feelings Lie to You (Sometimes)</a></strong> - My exploration of how emotions and logic shape our decision-making process</li>
  <li><strong><a href="/2025/09/21/coding-in-the-era-of-llms.html">Coding in the era of LLMs</a></strong> - My thoughts on AI-assisted coding and the importance of learning fundamentals</li>
</ul>]]></content><author><name>Bejay</name></author><category term="Tech" /><category term="Reflections" /><summary type="html"><![CDATA[Why life's complexities are best understood as weighted combinations of multiple factors, not single causes.]]></summary></entry><entry><title type="html">Coding in the era of LLMs</title><link href="http://localhost:4000/2025/09/21/coding-in-the-era-of-llms.html" rel="alternate" type="text/html" title="Coding in the era of LLMs" /><published>2025-09-21T00:00:00+05:30</published><updated>2025-09-21T00:00:00+05:30</updated><id>http://localhost:4000/2025/09/21/coding-in-the-era-of-llms</id><content type="html" xml:base="http://localhost:4000/2025/09/21/coding-in-the-era-of-llms.html"><![CDATA[<p>The common people began to have access to ‚ÄúAI‚Äù since the launch of chatgpt in 2022 end (30th Nov, 2022 to be precise).
Before that AI was on a sci-fi domain for people. AI was being used for very specific day to day tasks but it was not as significant or head turner as perhaps a Ultron of Avengers fame.</p>

<p>This opened a new avenue. People started recognising the true power of AI. This was also felt in the software engineering career. People started to write code using these LLMs.</p>

<p>I also joined the bandwagon. It definitely felt helpful for certain well defined tasks.
But AI assisted coding was far from perfect. But it steadily started improving.</p>

<p>The turning point for me was cursor - the AI assisted IDE. I really started growing fond of the tab feature. By then the models had started to improve drastically too. The claude sonnet 3.5 + cursor duo turned out to be really a great duo. The tab feature specifically works for me because it is I who take care of the logic while the tab feature autocompletes it. It makes coding faster.</p>

<p>Then came cursor agent. It could really start building features on its own.</p>

<h2 id="the-dark-side-nobody-talks-about">The Dark Side Nobody Talks About</h2>

<p>But here‚Äôs the thing that worries me as I see more people jumping into this AI-powered coding revolution: <strong>not everyone should be coding with AI the same way.</strong></p>

<p>There‚Äôs a dangerous trend emerging, especially among beginners. People who are brand new to programming are treating AI as a magic wand that removes the need to actually learn to code. They‚Äôre ‚Äúvibe coding‚Äù - throwing prompts at ChatGPT or Cursor and shipping whatever comes out without really understanding what‚Äôs happening under the hood.</p>

<h3 id="the-data-tells-a-sobering-story">The Data Tells a Sobering Story</h3>

<p>The numbers paint an uncomfortable picture. A 2025 study by METR tracked experienced developers and found something shocking: when using AI tools, developers actually took 19% longer to complete tasks compared to working without AI. Even more troubling? The developers thought they were 20% faster. That‚Äôs a 39-point gap between perception and reality.</p>

<p>Trust in AI coding tools has been plummeting too. Stack Overflow‚Äôs 2025 survey shows developer trust in AI output accuracy dropped from 43% in 2024 to just 33% in 2025. The favorability of adding AI tools to workflows fell from 72% to 60% in the same period.</p>

<h3 id="the-beginners-trap">The Beginner‚Äôs Trap</h3>

<p>For beginners, the risks are even more severe. When you don‚Äôt have the fundamentals, you can‚Äôt tell when AI is wrong. And it‚Äôs wrong more often than you‚Äôd think.</p>

<p>Consider these real problems people are facing:</p>

<ul>
  <li>Up to 30% of packages suggested by AI tools don‚Äôt even exist - they‚Äôre hallucinated, creating security vulnerabilities</li>
  <li>40% of AI-generated database queries are vulnerable to SQL injection attacks</li>
  <li>AI often puts security checks on the client side instead of the server</li>
  <li>Hardcoded API keys and secrets frequently appear in generated code</li>
</ul>

<p>One experienced developer shared how an AI-generated script locked them out of root access because they asked it to ‚Äúmake it super secure‚Äù without reviewing the implementation. These aren‚Äôt edge cases. They‚Äôre common patterns.</p>

<h3 id="you-still-need-to-learn-the-fundamentals">You Still Need to Learn the Fundamentals</h3>

<p>Here‚Äôs what the experts are saying: AI coding without a foundation is like letting someone who‚Äôs never flown a plane sit in the cockpit and take off in automated mode. It sounds crazy when you put it that way, right?</p>

<p>As one senior product manager at Qt Group put it: ‚ÄúIf junior developers generate code with AI assistants and deploy the code to digital products without truly understanding it, then they run into the risk of introducing suboptimal code. Whenever junior developers use AI-generated code, they‚Äôre not really learning how to write and review code themselves.‚Äù</p>

<p>The most successful approach isn‚Äôt to avoid AI - that would be career suicide at this point. According to a 2024 developer survey, 76% of developers are using or planning to use AI coding assistants. The market for AI coding tools was valued at $5.5 billion in 2024 and is projected to hit $47.3 billion by 2034.</p>

<p>But you need to <strong>earn the right</strong> to use these tools effectively.</p>

<h3 id="the-right-way-to-learn-with-ai">The Right Way to Learn with AI</h3>

<p>Think of AI as a senior developer looking over your shoulder, not as a replacement for your brain. Here‚Äôs what actually works:</p>

<ol>
  <li>
    <p><strong>Learn the fundamentals first</strong>. Understand variables, control flow, data structures, functions. Write enough code manually that you can spot when something looks wrong.</p>
  </li>
  <li>
    <p><strong>Write it yourself, then compare</strong>. Try solving a problem on your own first. Then ask AI how it would do it. See what‚Äôs different. Learn from the gaps.</p>
  </li>
  <li>
    <p><strong>Never deploy code you don‚Äôt understand</strong>. If AI generates something and you can‚Äôt explain what each part does, you‚Äôre not ready to use it. Debug it. Break it. Fix it. Make it yours.</p>
  </li>
  <li>
    <p><strong>Use AI to augment, not replace, learning</strong>. AI is incredible for explaining concepts, suggesting improvements, and catching bugs. But it can‚Äôt build your mental model of how code works.</p>
  </li>
</ol>

<p>The developers who will thrive aren‚Äôt the ones who can write the best prompts. They‚Äôre the ones who know enough to recognize when AI is leading them astray.</p>

<h3 id="looking-forward">Looking Forward</h3>

<p>AI coding tools are only going to get better. Models are improving, context windows are expanding (Cursor now offers 1M+ token context windows), and response times are getting faster. We‚Äôre seeing the emergence of autonomous coding agents that can handle entire features.</p>

<p>But the gap between those who understand what‚Äôs happening and those who are just prompt-engineering their way through is going to become a chasm. Companies are already learning what happens when their codebases get infiltrated with AI-generated code at scale. We‚Äôre seeing bigger incidents with slower resolution times because the people trying to fix problems don‚Äôt understand the code that created them.</p>

<h2 id="my-take">My Take</h2>

<p>I love AI coding tools. The cursor + claude combo has genuinely made me more productive. But I was coding since before these tools existed. I had spent years debugging obscure errors, refactoring messy code, and building that intuition for what good code looks like. That foundation is why I can tell when cursor‚Äôs suggestions are brilliant and when they‚Äôre subtly broken.</p>

<p>I see people jumping straight to AI without that foundation, and honestly, it makes me a bit nervous for them. Not in a gatekeeping way - I‚Äôm genuinely excited about more people getting into coding. But there‚Äôs a difference between using AI as a productivity booster and using it as a crutch to avoid learning.</p>

<p>When I use the tab feature, I already know what I want to build. The AI just saves me the typing. When I use cursor agent, I review every change it makes because I know what to look for. That‚Äôs the difference.</p>

<p>Look, I‚Äôm not going to tell you how to learn. Maybe you‚Äôll figure out your own path that works better. But from where I‚Äôm standing, having gone through both worlds - the pre-AI grind and the AI-assisted present - the people who seem to struggle the most are the ones who skipped straight to step two.</p>

<p>The cursor + claude duo is incredible. But it‚Äôs incredible <em>because</em> I know what I‚Äôm doing. Without that, it‚Äôs just a fancy autocomplete that occasionally leads you off a cliff.</p>

<p>That‚Äôs my experience anyway. Your mileage may vary.</p>

<hr />

<h2 id="more-from-my-blog">More from my blog</h2>

<ul>
  <li><strong><a href="/2025/10/20/frame-ai.html">Frame AI: Building an AI-Powered Photography Assistant</a></strong> - How I built an AI system that analyzes and enhances photos while teaching me</li>
  <li><strong><a href="/2025/04/15/your-feelings-lie-to-you-sometimes.html">Your Feelings Lie to You (Sometimes)</a></strong> - My exploration of how emotions and logic shape our decision-making process</li>
  <li><strong><a href="/2025/10/16/life-is-linear-regression.html">Everything in Life is Linear Regression</a></strong> - Why life‚Äôs complexities are best understood as weighted combinations of multiple factors</li>
</ul>]]></content><author><name>Bejay</name></author><category term="Tech" /><category term="Reflections" /><summary type="html"><![CDATA[My thought dump on AI and coding]]></summary></entry><entry><title type="html">Your Feelings Lie to You (Sometimes)</title><link href="http://localhost:4000/2025/04/15/your-feelings-lie-to-you-sometimes.html" rel="alternate" type="text/html" title="Your Feelings Lie to You (Sometimes)" /><published>2025-04-15T00:00:00+05:30</published><updated>2025-04-15T00:00:00+05:30</updated><id>http://localhost:4000/2025/04/15/your-feelings-lie-to-you-sometimes</id><content type="html" xml:base="http://localhost:4000/2025/04/15/your-feelings-lie-to-you-sometimes.html"><![CDATA[<h2 id="we-are-human-beings-we-feel-things-and-thats-beautiful">We are human beings. We feel things. And that‚Äôs beautiful.</h2>

<p>But we also have a gift from evolution called the prefrontal cortex, the logical center of the brain.
Pop culture constantly pushes the idea of ‚Äúfollow your heart‚Äù or ‚Äúdo what feels right.‚Äù
But honestly? That‚Äôs not always the best way to live.</p>

<p>I would always suggest: use data, write stuff down, and then make decisions.
After that, if you fail so be it.
At least you made the best possible decision with the information you had at the time.</p>

<h2 id="emotions-are-amazing-but">Emotions Are Amazing, But‚Ä¶</h2>

<p>Don‚Äôt get me wrong.
I‚Äôm not some uptight dude who only thinks logically 24/7.
Emotions are wonderful. They make us human. They give life meaning.</p>

<p>What we feel is real to us.
But at any given point, making informed decisions will serve us better.</p>

<p>Now, I‚Äôm not saying you need to freeze and over-analyze every tiny choice.
Instead, ask yourself a simple question:</p>

<blockquote>
  <p><strong>Is the decision easily reversible?</strong></p>

  <p>If yes, go ahead, act fast.</p>

  <p>If no, take a step back, breathe, and analyze properly.</p>
</blockquote>

<h2 id="when-you-dont-feel-like-it">When You Don‚Äôt Feel Like It</h2>

<p>Let‚Äôs look at another side of this.
Sometimes you don‚Äôt feel like doing something.
You might not feel motivated. You might not feel happy.
You might even want to do something‚Ä¶ but just not feel like it.</p>

<p>In those moments, you have to let your logical brain drive you even more.</p>

<p>Nowadays, almost everyone battles some form of anxiety or depression.
In those states, your feelings and your thoughts will lie to you.
You‚Äôll think you can‚Äôt. You‚Äôll feel like it‚Äôs pointless.</p>

<p>That‚Äôs exactly when practicing data-driven, logical action becomes your superpower.
I am not saying one should suppress emotions, it‚Äôs about not letting temporary feelings sabotage your trajectory in the long run.</p>

<h2 id="who-am-i">Who Am I?</h2>

<p>I‚Äôm a 28-year-old software engineer.
Here‚Äôs a quick snapshot of my journey:</p>

<ul>
  <li>Graduated B.Tech in ECE ‚Äî 2019</li>
  <li>Completed M.Tech during the COVID years ‚Äî 2021</li>
  <li>Started a PhD, dropped out after a year</li>
  <li>Prepared for interviews, hustled hard</li>
  <li>Finally joined a startup in October 2022</li>
</ul>

<p>It‚Äôs been a messy, beautiful ride.
And through it all, what I have learnt is to not trust my feelings blindly, but trust the process.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Feelings are important.
But for the most important decisions in life, let the logic lead.</p>

<p>Write things down.
Check the data.
Think for a minute longer.
And then act with all the conviction.</p>

<p>Feel your feelings, but don‚Äôt be ruled by them.</p>

<hr />

<h2 id="more-from-my-blog">More from my blog</h2>

<ul>
  <li><strong><a href="/2025/10/20/frame-ai.html">Frame AI: Building an AI-Powered Photography Assistant</a></strong> - How I built an AI system that analyzes and enhances photos while teaching me</li>
  <li><strong><a href="/2025/09/21/coding-in-the-era-of-llms.html">Coding in the era of LLMs</a></strong> - My thoughts on AI-assisted coding and the importance of learning fundamentals</li>
  <li><strong><a href="/2025/10/16/life-is-linear-regression.html">Everything in Life is Linear Regression</a></strong> - Why life‚Äôs complexities are best understood as weighted combinations of multiple factors</li>
</ul>]]></content><author><name>Bejay</name></author><category term="Reflections" /><summary type="html"><![CDATA[My exploration of how emotions and logic shape our decision-making process]]></summary></entry></feed>